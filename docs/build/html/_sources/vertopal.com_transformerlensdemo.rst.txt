.. container:: cell markdown

   .. rubric:: Transformer Lens Main Demo Notebook
      :name: transformer-lens-main-demo-notebook

   To use this notebook, go to Runtime > Change Runtime Type and select
   GPU as the hardware accelerator.

.. container:: cell markdown

   **Tips for reading this Colab:**

   -  You can run all this code for yourself!
   -  The graphs are interactive!
   -  Use the table of contents pane in the sidebar to navigate
   -  Collapse irrelevant sections with the dropdown arrows
   -  Search the page using the search in the sidebar, not CTRL+F

.. container:: cell markdown

   .. rubric:: Setup
      :name: setup

   (No need to read)

.. container:: cell code

   .. code:: python

      # Janky code to do different setup when run in a Colab notebook vs VSCode
      DEVELOPMENT_MODE = False
      try:
          import google.colab
          IN_COLAB = True
          print("Running as a Colab notebook")
          %pip install git+https://github.com/neelnanda-io/TransformerLens.git
          %pip install circuitsvis
          
          # PySvelte is an unmaintained visualization library, use it as a backup if circuitsvis isn't working
          # # Install another version of node that makes PySvelte work way faster
          # !curl -fsSL https://deb.nodesource.com/setup_16.x | sudo -E bash -; sudo apt-get install -y nodejs
          # %pip install git+https://github.com/neelnanda-io/PySvelte.git
      except:
          IN_COLAB = False
          print("Running as a Jupyter notebook - intended for development only!")
          from IPython import get_ipython

          ipython = get_ipython()
          # Code to automatically update the HookedTransformer code as its edited without restarting the kernel
          ipython.magic("load_ext autoreload")
          ipython.magic("autoreload 2")

   .. container:: output stream stdout

      ::

         Running as a Jupyter notebook - intended for development only!

.. container:: cell code

   .. code:: python

      # Plotly needs a different renderer for VSCode/Notebooks vs Colab argh
      import plotly.io as pio
      if IN_COLAB or not DEVELOPMENT_MODE:
          pio.renderers.default = "colab"
      else:
          pio.renderers.default = "notebook_connected"
      print(f"Using renderer: {pio.renderers.default}")

   .. container:: output stream stdout

      ::

         Using renderer: colab

.. container:: cell code

   .. code:: python

      import circuitsvis as cv
      # Testing that the library works
      cv.examples.hello("Neel")

   .. container:: output execute_result

      ::

         <circuitsvis.utils.render.RenderedHTML at 0x7f96ad0fdcd0>

.. container:: cell code

   .. code:: python

      # Import stuff
      import torch
      import torch.nn as nn
      import torch.nn.functional as F
      import torch.optim as optim
      import numpy as np
      import einops
      from fancy_einsum import einsum
      import tqdm.auto as tqdm
      import random
      from pathlib import Path
      import plotly.express as px
      from torch.utils.data import DataLoader

      from torchtyping import TensorType as TT
      from typing import List, Union, Optional
      from functools import partial
      import copy

      import itertools
      from transformers import AutoModelForCausalLM, AutoConfig, AutoTokenizer
      import dataclasses
      import datasets
      from IPython.display import HTML

.. container:: cell code

   .. code:: python

      import transformer_lens
      import transformer_lens.utils as utils
      from transformer_lens.hook_points import (
          HookedRootModule,
          HookPoint,
      )  # Hooking utilities
      from transformer_lens import HookedTransformer, HookedTransformerConfig, FactoredMatrix, ActivationCache

.. container:: cell markdown

   We turn automatic differentiation off, to save GPU memory, as this
   notebook focuses on model inference not model training.

.. container:: cell code

   .. code:: python

      torch.set_grad_enabled(False)

   .. container:: output execute_result

      ::

         <torch.autograd.grad_mode.set_grad_enabled at 0x7f973be365d0>

.. container:: cell markdown

   Plotting helper functions:

.. container:: cell code

   .. code:: python

      def imshow(tensor, renderer=None, xaxis="", yaxis="", **kwargs):
          px.imshow(utils.to_numpy(tensor), color_continuous_midpoint=0.0, color_continuous_scale="RdBu", labels={"x":xaxis, "y":yaxis}, **kwargs).show(renderer)

      def line(tensor, renderer=None, xaxis="", yaxis="", **kwargs):
          px.line(utils.to_numpy(tensor), labels={"x":xaxis, "y":yaxis}, **kwargs).show(renderer)

      def scatter(x, y, xaxis="", yaxis="", caxis="", renderer=None, **kwargs):
          x = utils.to_numpy(x)
          y = utils.to_numpy(y)
          px.scatter(y=y, x=x, labels={"x":xaxis, "y":yaxis, "color":caxis}, **kwargs).show(renderer)

.. container:: cell markdown

   .. rubric:: Introduction
      :name: introduction

.. container:: cell markdown

   This is a demo notebook for
   `TransformerLens <https://github.com/neelnanda-io/TransformerLens>`__,
   **a library I (**\ `Neel Nanda <neelnanda.io>`__\ **) wrote for
   doing**\ `mechanistic
   interpretability <https://distill.pub/2020/circuits/zoom-in/>`__\ **of
   GPT-2 Style language models.** The goal of mechanistic
   interpretability is to take a trained model and reverse engineer the
   algorithms the model learned during training from its weights. It is
   a fact about the world today that we have computer programs that can
   essentially speak English at a human level (GPT-3, PaLM, etc), yet we
   have no idea how they work nor how to write one ourselves. This
   offends me greatly, and I would like to solve this! Mechanistic
   interpretability is a very young and small field, and there are a
   *lot* of open problems - if you would like to help, please try
   working on one! **If you want to skill up, check out**\ `my guide to
   getting started <https://neelnanda.io/getting-started>`__\ **, and if
   you want to jump into an open problem check out my sequence**\ `200
   Concrete Open Problems in Mechanistic
   Interpretability <https://neelnanda.io/concrete-open-problems>`__\ **.**

   I wrote this library because after I left the Anthropic
   interpretability team and started doing independent research, I got
   extremely frustrated by the state of open source tooling. There's a
   lot of excellent infrastructure like HuggingFace and DeepSpeed to
   *use* or *train* models, but very little to dig into their internals
   and reverse engineer how they work. **This library tries to solve
   that**, and to make it easy to get into the field even if you don't
   work at an industry org with real infrastructure! The core features
   were heavily inspired by `Anthropic's excellent Garcon
   tool <https://transformer-circuits.pub/2021/garcon/index.html>`__.
   Credit to Nelson Elhage and Chris Olah for building Garcon and
   showing me the value of good infrastructure for accelerating
   exploratory research!

   The core design principle I've followed is to enable exploratory
   analysis - one of the most fun parts of mechanistic interpretability
   compared to normal ML is the extremely short feedback loops! The
   point of this library is to keep the gap between having an experiment
   idea and seeing the results as small as possible, to make it easy for
   **research to feel like play** and to enter a flow state. This
   notebook demonstrates how the library works and how to use it, but if
   you want to see how well it works for exploratory research, check out
   `my notebook analysing Indirect Objection
   Identification <https://neelnanda.io/exploratory-analysis-demo>`__ or
   `my recording of myself doing
   research <https://www.youtube.com/watch?v=yo4QvDn-vsU>`__!

.. container:: cell markdown

   .. rubric:: Loading and Running Models
      :name: loading-and-running-models

   TransformerLens comes loaded with >40 open source GPT-style models.
   You can load any of them in with
   ``HookedTransformer.from_pretrained(MODEL_NAME)``. For this demo
   notebook we'll look at GPT-2 Small, an 80M parameter model, see the
   Available Models section for info on the rest.

.. container:: cell code

   .. code:: python

      device = "cuda" if torch.cuda.is_available() else "cpu"

.. container:: cell code

   .. code:: python

      model = HookedTransformer.from_pretrained("gpt2-small", device=device)

   .. container:: output stream stderr

      ::

         Using pad_token, but it is not set yet.

   .. container:: output stream stdout

      ::

         Loaded pretrained model gpt2-small into HookedTransformer

.. container:: cell markdown

   To try the model the model out, let's find the loss on this text!
   Models can be run on a single string or a tensor of tokens (shape:
   [batch, position], all integers), and the possible return types are:

   -  "logits" (shape [batch, position, d_vocab], floats),
   -  "loss" (the cross-entropy loss when predicting the next token),
   -  "both" (a tuple of (logits, loss))
   -  None (run the model, but don't calculate the logits - this is
      faster when we only want to use intermediate activations)

.. container:: cell code

   .. code:: python

      model_description_text = """## Loading Models

      HookedTransformer comes loaded with >40 open source GPT-style models. You can load any of them in with `HookedTransformer.from_pretrained(MODEL_NAME)`. See my explainer for documentation of all supported models, and this table for hyper-parameters and the name used to load them. Each model is loaded into the consistent HookedTransformer architecture, designed to be clean, consistent and interpretability-friendly. 

      For this demo notebook we'll look at GPT-2 Small, an 80M parameter model. To try the model the model out, let's find the loss on this paragraph!"""
      loss = model(model_description_text, return_type="loss")
      print("Model loss:", loss)

   .. container:: output stream stdout

      ::

         Model loss: tensor(4.1653, device='cuda:0')

.. container:: cell markdown

   .. rubric:: Caching all Activations
      :name: caching-all-activations

   The first basic operation when doing mechanistic interpretability is
   to break open the black box of the model and look at all of the
   internal activations of a model. This can be done with
   ``logits, cache = model.run_with_cache(tokens)``. Let's try this out
   on the first line of the abstract of the GPT-2 paper.

   .. raw:: html

      <details>

   .. raw:: html

      <summary>On `remove_batch_dim`</summary>

   Every activation inside the model begins with a batch dimension.
   Here, because we only entered a single batch dimension, that
   dimension is always length 1 and kinda annoying, so passing in the
   ``remove_batch_dim=True`` keyword removes it.
   ``gpt2_cache_no_batch_dim = gpt2_cache.remove_batch_dim()`` would
   have achieved the same effect. </details?>

.. container:: cell code

   .. code:: python

      gpt2_text = "Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets."
      gpt2_tokens = model.to_tokens(gpt2_text)
      print(gpt2_tokens.device)
      gpt2_logits, gpt2_cache = model.run_with_cache(gpt2_tokens, remove_batch_dim=True)

   .. container:: output stream stdout

      ::

         cuda:0

.. container:: cell markdown

   Let's visualize the attention pattern of all the heads in layer 0,
   using `Alan Cooney's CircuitsVis
   library <https://github.com/alan-cooney/CircuitsVis>`__ (based on
   `Anthropic's PySvelte
   library <https://github.com/anthropics/PySvelte>`__).

   We look this the attention pattern in ``gpt2_cache``, an
   ``ActivationCache`` object, by entering in the name of the
   activation, followed by the layer index (here, the activation is
   called "attn" and the layer index is 0). This has shape [head_index,
   destination_position, source_position], and we use the
   ``model.to_str_tokens`` method to convert the text to a list of
   tokens as strings, since there is an attention weight between each
   pair of tokens.

   This visualization is interactive! Try hovering over a token or head,
   and click to lock. The grid on the top left and for each head is the
   attention pattern as a destination position by source position grid.
   It's lower triangular because GPT-2 has **causal attention**,
   attention can only look backwards, so information can only move
   forwards in the network.

   See the ActivationCache section for more on what ``gpt2_cache`` can
   do.

.. container:: cell code

   .. code:: python

      print(type(gpt2_cache))
      attention_pattern = gpt2_cache["pattern", 0, "attn"]
      print(attention_pattern.shape)
      gpt2_str_tokens = model.to_str_tokens(gpt2_text)

   .. container:: output stream stdout

      ::

         <class 'transformer_lens.ActivationCache.ActivationCache'>
         torch.Size([12, 33, 33])

.. container:: cell code

   .. code:: python

      print("Layer 0 Head Attention Patterns:")
      cv.attention.attention_patterns(tokens=gpt2_str_tokens, attention=attention_pattern)

   .. container:: output stream stdout

      ::

         Layer 0 Head Attention Patterns:
         huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
         To disable this warning, you can either:
         	- Avoid using `tokenizers` before the fork if possible
         	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)

   .. container:: output execute_result

      ::

         <circuitsvis.utils.render.RenderedHTML at 0x7f96781e7790>

.. container:: cell markdown

   .. rubric:: Hooks: Intervening on Activations
      :name: hooks-intervening-on-activations

.. container:: cell markdown

   One of the great things about interpreting neural networks is that we
   have *full control* over our system. From a computational
   perspective, we know exactly what operations are going on inside
   (even if we don't know what they mean!). And we can make precise,
   surgical edits and see how the model's behaviour and other internals
   change. This is an extremely powerful tool, because it can let us eg
   set up careful counterfactuals and causal intervention to easily
   understand model behaviour.

   Accordingly, being able to do this is a pretty core operation, and
   this is one of the main things TransformerLens supports! The key
   feature here is **hook points**. Every activation inside the
   transformer is surrounded by a hook point, which allows us to edit or
   intervene on it.

   We do this by adding a **hook function** to that activation. The hook
   function maps ``current_activation_value, hook_point`` to
   ``new_activation_value``. As the model is run, it computes that
   activation as normal, and then the hook function is applied to
   compute a replacement, and that is substituted in for the activation.
   The hook function can be an arbitrary Python function, so long as it
   returns a tensor of the correct shape.

   .. raw:: html

      <details><summary>Relationship to PyTorch hooks</summary>

      [PyTorch hooks](https://blog.paperspace.com/pytorch-hooks-gradient-clipping-debugging/) are a great and underrated, yet incredibly janky, feature. They can act on a layer, and edit the input or output of that layer, or the gradient when applying autodiff. The key difference is that **Hook points** act on *activations* not layers. This means that you can intervene within a layer on each activation, and don't need to care about the precise layer structure of the transformer. And it's immediately clear exactly how the hook's effect is applied. This adjustment was shamelessly inspired by [Garcon's use of ProbePoints](https://transformer-circuits.pub/2021/garcon/index.html).

      They also come with a range of other quality of life improvements, like the model having a `model.reset_hooks()` method to remove all hooks, or helper methods to temporarily add hooks for a single forward pass - it is *incredibly* easy to shoot yourself in the foot with standard PyTorch hooks!
      </details>

.. container:: cell markdown

   As a basic example, let's
   `ablate <https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=fh-HJyz1CgUVrXuoiban6bYx>`__
   head 7 in layer 0 on the text above.

   We define a ``head_ablation_hook`` function. This takes the value
   tensor for attention layer 0, and sets the component with
   ``head_index==7`` to zero and returns it (Note - we return by
   convention, but since we're editing the activation in-place, we don't
   strictly *need* to).

   We then use the ``run_with_hooks`` helper function to run the model
   and *temporarily* add in the hook for just this run. We enter in the
   hook as a tuple of the activation name (also the hook point name -
   found with ``utils.get_act_name``) and the hook function.

.. container:: cell code

   .. code:: python

      layer_to_ablate = 0
      head_index_to_ablate = 8

      # We define a head ablation hook
      # The type annotations are NOT necessary, they're just a useful guide to the reader
      # 
      def head_ablation_hook(
          value: TT["batch", "pos", "head_index", "d_head"],
          hook: HookPoint
      ) -> TT["batch", "pos", "head_index", "d_head"]:
          print(f"Shape of the value tensor: {value.shape}")
          value[:, :, head_index_to_ablate, :] = 0.
          return value

      original_loss = model(gpt2_tokens, return_type="loss")
      ablated_loss = model.run_with_hooks(
          gpt2_tokens, 
          return_type="loss", 
          fwd_hooks=[(
              utils.get_act_name("v", layer_to_ablate), 
              head_ablation_hook
              )]
          )
      print(f"Original Loss: {original_loss.item():.3f}")
      print(f"Ablated Loss: {ablated_loss.item():.3f}")

   .. container:: output stream stdout

      ::

         Shape of the value tensor: torch.Size([1, 33, 12, 64])
         Original Loss: 3.999
         Ablated Loss: 4.117

.. container:: cell markdown

   **Gotcha:** Hooks are global state - they're added in as part of the
   model, and stay there until removed. ``run_with_hooks`` tries to
   create an abstraction where these are local state, by removing all
   hooks at the end of the function. But you can easily shoot yourself
   in the foot if there's, eg, an error in one of your hooks so the
   function never finishes. If you start getting bugs, try
   ``model.reset_hooks()`` to clean things up. Further, if you *do* add
   hooks of your own that you want to keep, which you can do with
   ``add_hook`` on the relevant

.. container:: cell markdown

   .. rubric:: Activation Patching on the Indirect Object Identification
      Task
      :name: activation-patching-on-the-indirect-object-identification-task

.. container:: cell markdown

   For a somewhat more involved example, let's use hooks to apply
   `activation
   patching <https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=qeWBvs-R-taFfcCq-S_hgMqx>`__
   on the `Indirect Object
   Identification <https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=iWsV3s5Kdd2ca3zNgXr5UPHa>`__
   (IOI) task.

   The IOI task is the task of identifying that a sentence like "After
   John and Mary went to the store, Mary gave a bottle of milk to" with
   " John" rather than " Mary" (ie, finding the indirect object), and
   Redwood Research have `an excellent paper studying the underlying
   circuit in GPT-2 Small <https://arxiv.org/abs/2211.00593>`__.

   `Activation
   patching <https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=qeWBvs-R-taFfcCq-S_hgMqx>`__
   is a technique from `Kevin Meng and David Bau's excellent ROME
   paper <https://rome.baulab.info/>`__. The goal is to identify which
   model activations are important for completing a task. We do this by
   setting up a **clean prompt** and a **corrupted prompt** and a
   **metric** for performance on the task. We then pick a specific model
   activation, run the model on the corrupted prompt, but then
   *intervene* on that activation and patch in its value when run on the
   clean prompt. We then apply the metric, and see how much this patch
   has recovered the clean performance. (See `a more detailed
   demonstration of activation patching
   here <https://colab.research.google.com/github/neelnanda-io/Easy-Transformer/blob/main/Exploratory_Analysis_Demo.ipynb#scrollTo=5nUXG6zqmd0f>`__)

.. container:: cell markdown

   Here, our clean prompt is "After John and Mary went to the store,
   **Mary** gave a bottle of milk to", our corrupted prompt is "After
   John and Mary went to the store, **John** gave a bottle of milk to",
   and our metric is the difference between the correct logit ( John)
   and the incorrect logit ( Mary) on the final token.

   We see that the logit difference is significantly positive on the
   clean prompt, and significantly negative on the corrupted prompt,
   showing that the model is capable of doing the task!

.. container:: cell code

   .. code:: python

      clean_prompt = "After John and Mary went to the store, Mary gave a bottle of milk to"
      corrupted_prompt = "After John and Mary went to the store, John gave a bottle of milk to"

      clean_tokens = model.to_tokens(clean_prompt)
      corrupted_tokens = model.to_tokens(corrupted_prompt)

      def logits_to_logit_diff(logits, correct_answer=" John", incorrect_answer=" Mary"):
          # model.to_single_token maps a string value of a single token to the token index for that token
          # If the string is not a single token, it raises an error.
          correct_index = model.to_single_token(correct_answer)
          incorrect_index = model.to_single_token(incorrect_answer)
          return logits[0, -1, correct_index] - logits[0, -1, incorrect_index]

      # We run on the clean prompt with the cache so we store activations to patch in later.
      clean_logits, clean_cache = model.run_with_cache(clean_tokens)
      clean_logit_diff = logits_to_logit_diff(clean_logits)
      print(f"Clean logit difference: {clean_logit_diff.item():.3f}")

      # We don't need to cache on the corrupted prompt.
      corrupted_logits = model(corrupted_tokens)
      corrupted_logit_diff = logits_to_logit_diff(corrupted_logits)
      print(f"Corrupted logit difference: {corrupted_logit_diff.item():.3f}")

   .. container:: output stream stdout

      ::

         Clean logit difference: 4.276
         Corrupted logit difference: -2.738

.. container:: cell markdown

   We now setup the hook function to do activation patching. Here, we'll
   patch in the `residual
   stream <https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=DHp9vZ0h9lA9OCrzG2Y3rrzH>`__
   at the start of a specific layer and at a specific position. This
   will let us see how much the model is using the residual stream at
   that layer and position to represent the key information for the
   task.

   We want to iterate over all layers and positions, so we write the
   hook to take in an position parameter. Hook functions must have the
   input signature (activation, hook), but we can use
   ``functools.partial`` to set the position parameter before passing it
   to ``run_with_hooks``

.. container:: cell code

   .. code:: python

      # We define a residual stream patching hook
      # We choose to act on the residual stream at the start of the layer, so we call it resid_pre
      # The type annotations are a guide to the reader and are not necessary
      def residual_stream_patching_hook(
          resid_pre: TT["batch", "pos", "d_model"],
          hook: HookPoint,
          position: int
      ) -> TT["batch", "pos", "d_model"]:
          # Each HookPoint has a name attribute giving the name of the hook.
          clean_resid_pre = clean_cache[hook.name]
          resid_pre[:, position, :] = clean_resid_pre[:, position, :]
          return resid_pre

      # We make a tensor to store the results for each patching run. We put it on the model's device to avoid needing to move things between the GPU and CPU, which can be slow.
      num_positions = len(clean_tokens[0])
      ioi_patching_result = torch.zeros((model.cfg.n_layers, num_positions), device=model.cfg.device)

      for layer in tqdm.tqdm(range(model.cfg.n_layers)):
          for position in range(num_positions):
              # Use functools.partial to create a temporary hook function with the position fixed
              temp_hook_fn = partial(residual_stream_patching_hook, position=position)
              # Run the model with the patching hook
              patched_logits = model.run_with_hooks(corrupted_tokens, fwd_hooks=[
                  (utils.get_act_name("resid_pre", layer), temp_hook_fn)
              ])
              # Calculate the logit difference
              patched_logit_diff = logits_to_logit_diff(patched_logits).detach()
              # Store the result, normalizing by the clean and corrupted logit difference so it's between 0 and 1 (ish)
              ioi_patching_result[layer, position] = (patched_logit_diff - corrupted_logit_diff)/(clean_logit_diff - corrupted_logit_diff)

   .. container:: output display_data

      .. code:: json

         {"version_major":2,"version_minor":0,"model_id":"69225f8c0e1949b8a900aa73abcdd5d4"}

.. container:: cell markdown

   We can now visualize the results, and see that this computation is
   extremely localised within the model. Initially, the second subject
   (Mary) token is all that matters (naturally, as it's the only
   different token), and all relevant information remains here until
   heads in layer 7 and 8 move this to the final token where it's used
   to predict the indirect object. (Note - the heads are in layer 7 and
   8, not 8 and 9, because we patched in the residual stream at the
   *start* of each layer)

.. container:: cell code

   .. code:: python

      # Add the index to the end of the label, because plotly doesn't like duplicate labels
      token_labels = [f"{token}_{index}" for index, token in enumerate(model.to_str_tokens(clean_tokens))]
      imshow(ioi_patching_result, x=token_labels, xaxis="Position", yaxis="Layer", title="Normalized Logit Difference After Patching Residual Stream on the IOI Task")

   .. container:: output display_data

      .. raw:: html

         <html>
         <head><meta charset="utf-8" /></head>
         <body>
             <div>            <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG"></script><script type="text/javascript">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: "STIX-Web"}});}</script>                <script type="text/javascript">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>
                 <script src="https://cdn.plot.ly/plotly-2.16.1.min.js"></script>                <div id="870151d1-9468-423f-a1cd-d18f40f05d5a" class="plotly-graph-div" style="height:525px; width:100%;"></div>            <script type="text/javascript">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById("870151d1-9468-423f-a1cd-d18f40f05d5a")) {                    Plotly.newPlot(                        "870151d1-9468-423f-a1cd-d18f40f05d5a",                        [{"coloraxis":"coloraxis","name":"0","x":["<|endoftext|>_0","After_1"," John_2"," and_3"," Mary_4"," went_5"," to_6"," the_7"," store_8",",_9"," Mary_10"," gave_11"," a_12"," bottle_13"," of_14"," milk_15"," to_16"],"z":[[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.9981473,0.0015992423,0.00014874776,-0.00037146147,-0.000021890666,-0.0006287108,-0.00051490654],[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.99805623,0.0022832917,0.00018165173,-0.00050525286,-0.00026907842,-0.000051667408,-0.0012817597],[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.9967364,0.004080638,0.0009735228,0.000042693595,-0.00015935316,-0.0003361101,-0.0019451419],[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.9905891,0.0199874,0.0018948342,0.0010126812,-0.0000681194,0.0009104341,-0.0019014967],[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.96165043,0.0853464,0.005204132,0.0030520482,0.00019565632,0.0011059545,-0.0022847871],[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.9630986,0.0843707,0.004121428,0.0007170893,0.00010265499,0.0010024837,-0.0042156526],[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.93591666,0.11111593,0.0077038826,0.00037540452,0.00036411927,0.0013256769,0.018745072],[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.770154,0.037418216,0.0020668323,-0.00008321172,0.00013419929,0.0017248756,0.4499055],[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.09650534,0.025926434,0.0019708397,0.00032849595,0.0004234008,0.0018843648,0.8994715],[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.02332294,0.018537723,0.0015863255,0.00052591984,0.0002523545,0.0008729073,0.96127474],[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.008559114,0.006339591,0.0005803066,-0.00034277246,0.0001091814,0.000648426,0.9495808]],"type":"heatmap","xaxis":"x","yaxis":"y","hovertemplate":"Position: %{x}<br>Layer: %{y}<br>color: %{z}<extra></extra>"}],                        {"template":{"data":{"histogram2dcontour":[{"type":"histogram2dcontour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"choropleth":[{"type":"choropleth","colorbar":{"outlinewidth":0,"ticks":""}}],"histogram2d":[{"type":"histogram2d","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmap":[{"type":"heatmap","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmapgl":[{"type":"heatmapgl","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"contourcarpet":[{"type":"contourcarpet","colorbar":{"outlinewidth":0,"ticks":""}}],"contour":[{"type":"contour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"surface":[{"type":"surface","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"mesh3d":[{"type":"mesh3d","colorbar":{"outlinewidth":0,"ticks":""}}],"scatter":[{"fillpattern":{"fillmode":"overlay","size":10,"solidity":0.2},"type":"scatter"}],"parcoords":[{"type":"parcoords","line":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolargl":[{"type":"scatterpolargl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"scattergeo":[{"type":"scattergeo","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolar":[{"type":"scatterpolar","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"scattergl":[{"type":"scattergl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatter3d":[{"type":"scatter3d","line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattermapbox":[{"type":"scattermapbox","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterternary":[{"type":"scatterternary","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattercarpet":[{"type":"scattercarpet","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"pie":[{"automargin":true,"type":"pie"}]},"layout":{"autotypenumbers":"strict","colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"hovermode":"closest","hoverlabel":{"align":"left"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"bgcolor":"#E5ECF6","angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"ternary":{"bgcolor":"#E5ECF6","aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"sequential":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"sequentialminus":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]]},"xaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"yaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"geo":{"bgcolor":"white","landcolor":"#E5ECF6","subunitcolor":"white","showland":true,"showlakes":true,"lakecolor":"white"},"title":{"x":0.05},"mapbox":{"style":"light"}}},"xaxis":{"anchor":"y","domain":[0.0,1.0],"scaleanchor":"y","constrain":"domain","title":{"text":"Position"}},"yaxis":{"anchor":"x","domain":[0.0,1.0],"autorange":"reversed","constrain":"domain","title":{"text":"Layer"}},"coloraxis":{"colorscale":[[0.0,"rgb(103,0,31)"],[0.1,"rgb(178,24,43)"],[0.2,"rgb(214,96,77)"],[0.3,"rgb(244,165,130)"],[0.4,"rgb(253,219,199)"],[0.5,"rgb(247,247,247)"],[0.6,"rgb(209,229,240)"],[0.7,"rgb(146,197,222)"],[0.8,"rgb(67,147,195)"],[0.9,"rgb(33,102,172)"],[1.0,"rgb(5,48,97)"]],"cmid":0.0},"title":{"text":"Normalized Logit Difference After Patching Residual Stream on the IOI Task"}},                        {"responsive": true}                    ).then(function(){
                                     
         var gd = document.getElementById('870151d1-9468-423f-a1cd-d18f40f05d5a');
         var x = new MutationObserver(function (mutations, observer) {{
                 var display = window.getComputedStyle(gd).display;
                 if (!display || display === 'none') {{
                     console.log([gd, 'removed!']);
                     Plotly.purge(gd);
                     observer.disconnect();
                 }}
         }});

         // Listen for the removal of the full notebook cells
         var notebookContainer = gd.closest('#notebook-container');
         if (notebookContainer) {{
             x.observe(notebookContainer, {childList: true});
         }}

         // Listen for the clearing of the current output cell
         var outputEl = gd.closest('.output');
         if (outputEl) {{
             x.observe(outputEl, {childList: true});
         }}

                                 })                };                            </script>        </div>
         </body>
         </html>

.. container:: cell markdown

   .. rubric:: Hooks: Accessing Activations
      :name: hooks-accessing-activations

.. container:: cell markdown

   Hooks can also be used to just **access** an activation - to run some
   function using that activation value, *without* changing the
   activation value. This can be achieved by just having the hook return
   nothing, and not editing the activation in place.

   This is useful for eg extracting activations for a specific task, or
   for doing some long-running calculation across many inputs, eg
   finding the text that most activates a specific neuron. (Note -
   everything this can do *could* be done with ``run_with_cache`` and
   post-processing, but this workflow can be more intuitive and memory
   efficient.)

.. container:: cell markdown

   To demonstrate this, let's look for `induction
   heads <https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html>`__
   in GPT-2 Small.

   Induction circuits are a very important circuit in generative
   language models, which are used to detect and continue repeated
   subsequences. They consist of two heads in separate layers that
   compose together, a **previous token head** which always attends to
   the previous token, and an **induction head** which attends to the
   token *after* an earlier copy of the current token.

   To see why this is important, let's say that the model is trying to
   predict the next token in a news article about Michael Jordan. The
   token " Michael", in general, could be followed by many surnames. But
   an induction head will look from that occurence of " Michael" to the
   token after previous occurences of " Michael", ie " Jordan" and can
   confidently predict that that will come next.

.. container:: cell markdown

   An interesting fact about induction heads is that they generalise to
   arbitrary sequences of repeated tokens. We can see this by generating
   sequences of 50 random tokens, repeated twice, and plotting the
   average loss at predicting the next token, by position. We see that
   the model goes from terrible to very good at the halfway point.

.. container:: cell code

   .. code:: python

      batch_size = 10
      seq_len = 50
      random_tokens = torch.randint(1000, 10000, (batch_size, seq_len)).to(model.cfg.device)
      repeated_tokens = einops.repeat(random_tokens, "batch seq_len -> batch (2 seq_len)")
      repeated_logits = model(repeated_tokens)
      correct_log_probs = model.loss_fn(repeated_logits, repeated_tokens, per_token=True)
      loss_by_position = einops.reduce(correct_log_probs, "batch position -> position", "mean")
      line(loss_by_position, xaxis="Position", yaxis="Loss", title="Loss by position on random repeated tokens")

   .. container:: output display_data

      .. raw:: html

         <html>
         <head><meta charset="utf-8" /></head>
         <body>
             <div>            <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG"></script><script type="text/javascript">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: "STIX-Web"}});}</script>                <script type="text/javascript">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>
                 <script src="https://cdn.plot.ly/plotly-2.16.1.min.js"></script>                <div id="23997d52-9026-4b2f-a2d9-3fcb18f25f86" class="plotly-graph-div" style="height:525px; width:100%;"></div>            <script type="text/javascript">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById("23997d52-9026-4b2f-a2d9-3fcb18f25f86")) {                    Plotly.newPlot(                        "23997d52-9026-4b2f-a2d9-3fcb18f25f86",                        [{"hovertemplate":"variable=0<br>index=%{x}<br>value=%{y}<extra></extra>","legendgroup":"0","line":{"color":"#636efa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"lines","name":"0","orientation":"v","showlegend":true,"x":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98],"xaxis":"x","y":[11.808037,13.792714,14.186319,13.627494,14.390996,13.2580385,13.232424,14.083632,12.428126,14.747475,12.65393,13.364669,11.224579,12.586921,11.750403,13.577395,12.499324,11.042871,12.462125,11.633731,11.359285,10.327721,11.381,11.717972,13.026313,12.365901,11.195658,11.747077,11.88308,12.5816145,11.464409,11.552933,11.633736,11.602069,11.794631,11.068112,11.379116,11.851734,12.331985,11.134096,11.409497,11.293555,11.135362,11.946617,11.007915,10.514901,11.653691,11.111884,11.219514,11.112963,3.0439572,1.049409,0.44605765,0.5474635,0.38069218,0.460846,0.18914191,0.19308324,0.38565096,0.24448323,0.059085872,0.28377718,0.4043172,0.3211415,0.07000212,0.16587926,0.080507755,0.08739778,0.187626,0.076328576,0.040779006,0.04520912,0.048187602,0.14715861,0.17393535,0.08196111,0.27518013,0.07813724,0.052669484,0.15392956,0.006187316,0.04051802,0.10128882,0.0450465,0.013996144,0.0148817245,0.040060904,0.100752585,0.06389619,0.06546464,0.043319672,1.0215524,0.075653516,0.056345176,0.12186723,0.03617762,0.15633447,0.20053823,0.047156777],"yaxis":"y","type":"scatter"}],                        {"template":{"data":{"histogram2dcontour":[{"type":"histogram2dcontour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"choropleth":[{"type":"choropleth","colorbar":{"outlinewidth":0,"ticks":""}}],"histogram2d":[{"type":"histogram2d","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmap":[{"type":"heatmap","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmapgl":[{"type":"heatmapgl","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"contourcarpet":[{"type":"contourcarpet","colorbar":{"outlinewidth":0,"ticks":""}}],"contour":[{"type":"contour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"surface":[{"type":"surface","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"mesh3d":[{"type":"mesh3d","colorbar":{"outlinewidth":0,"ticks":""}}],"scatter":[{"fillpattern":{"fillmode":"overlay","size":10,"solidity":0.2},"type":"scatter"}],"parcoords":[{"type":"parcoords","line":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolargl":[{"type":"scatterpolargl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"scattergeo":[{"type":"scattergeo","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolar":[{"type":"scatterpolar","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"scattergl":[{"type":"scattergl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatter3d":[{"type":"scatter3d","line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattermapbox":[{"type":"scattermapbox","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterternary":[{"type":"scatterternary","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattercarpet":[{"type":"scattercarpet","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"pie":[{"automargin":true,"type":"pie"}]},"layout":{"autotypenumbers":"strict","colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"hovermode":"closest","hoverlabel":{"align":"left"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"bgcolor":"#E5ECF6","angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"ternary":{"bgcolor":"#E5ECF6","aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"sequential":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"sequentialminus":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]]},"xaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"yaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"geo":{"bgcolor":"white","landcolor":"#E5ECF6","subunitcolor":"white","showland":true,"showlakes":true,"lakecolor":"white"},"title":{"x":0.05},"mapbox":{"style":"light"}}},"xaxis":{"anchor":"y","domain":[0.0,1.0],"title":{"text":"index"}},"yaxis":{"anchor":"x","domain":[0.0,1.0],"title":{"text":"value"}},"legend":{"title":{"text":"variable"},"tracegroupgap":0},"title":{"text":"Loss by position on random repeated tokens"}},                        {"responsive": true}                    ).then(function(){
                                     
         var gd = document.getElementById('23997d52-9026-4b2f-a2d9-3fcb18f25f86');
         var x = new MutationObserver(function (mutations, observer) {{
                 var display = window.getComputedStyle(gd).display;
                 if (!display || display === 'none') {{
                     console.log([gd, 'removed!']);
                     Plotly.purge(gd);
                     observer.disconnect();
                 }}
         }});

         // Listen for the removal of the full notebook cells
         var notebookContainer = gd.closest('#notebook-container');
         if (notebookContainer) {{
             x.observe(notebookContainer, {childList: true});
         }}

         // Listen for the clearing of the current output cell
         var outputEl = gd.closest('.output');
         if (outputEl) {{
             x.observe(outputEl, {childList: true});
         }}

                                 })                };                            </script>        </div>
         </body>
         </html>

.. container:: cell markdown

   The induction heads will be attending from the second occurence of
   each token to the token *after* its first occurence, ie the token
   ``50-1==49`` places back. So by looking at the average attention paid
   49 tokens back, we can identify induction heads! Let's define a hook
   to do this!

   .. raw:: html

      <details><summary>Technical details</summary>

      * We attach the hook to the attention pattern activation. There's one big pattern activation per layer, stacked across all heads, so we need to do some tensor manipulation to get a per-head score. 
      * Hook functions can access global state, so we make a big tensor to store the induction head score for each head, and then we just add the score for each head to the appropriate position in the tensor. 
      * To get a single hook function that works for each layer, we use the `hook.layer()` method to get the layer index (internally this is just inferred from the hook names).
      * As we want to add this to *every* activation pattern hook point, rather than giving the string for an activation name, this time we give a **name filter**. This is a Boolean function on hook point names, and it adds the hook function to every hook point where the function evaluates as true. 
          * `run_with_hooks` allows us to enter a list of (act_name, hook_function) pairs to all be added at once, so we could also have done this by inputting a list with a hook for each layer.
      </details>

.. container:: cell code

   .. code:: python

      # We make a tensor to store the induction score for each head. We put it on the model's device to avoid needing to move things between the GPU and CPU, which can be slow.
      induction_score_store = torch.zeros((model.cfg.n_layers, model.cfg.n_heads), device=model.cfg.device)
      def induction_score_hook(
          pattern: TT["batch", "head_index", "dest_pos", "source_pos"],
          hook: HookPoint,
      ):
          # We take the diagonal of attention paid from each destination position to source positions seq_len-1 tokens back
          # (This only has entries for tokens with index>=seq_len)
          induction_stripe = pattern.diagonal(dim1=-2, dim2=-1, offset=1-seq_len)
          # Get an average score per head
          induction_score = einops.reduce(induction_stripe, "batch head_index position -> head_index", "mean")
          # Store the result.
          induction_score_store[hook.layer(), :] = induction_score

      # We make a boolean filter on activation names, that's true only on attention pattern names.
      pattern_hook_names_filter = lambda name: name.endswith("pattern")

      model.run_with_hooks(
          repeated_tokens, 
          return_type=None, # For efficiency, we don't need to calculate the logits
          fwd_hooks=[(
              pattern_hook_names_filter,
              induction_score_hook
          )]
      )

      imshow(induction_score_store, xaxis="Head", yaxis="Layer", title="Induction Score by Head")

   .. container:: output display_data

      .. raw:: html

         <html>
         <head><meta charset="utf-8" /></head>
         <body>
             <div>            <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG"></script><script type="text/javascript">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: "STIX-Web"}});}</script>                <script type="text/javascript">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>
                 <script src="https://cdn.plot.ly/plotly-2.16.1.min.js"></script>                <div id="0351729d-f7bd-4dff-8e0c-0159e788d6eb" class="plotly-graph-div" style="height:525px; width:100%;"></div>            <script type="text/javascript">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById("0351729d-f7bd-4dff-8e0c-0159e788d6eb")) {                    Plotly.newPlot(                        "0351729d-f7bd-4dff-8e0c-0159e788d6eb",                        [{"coloraxis":"coloraxis","name":"0","z":[[0.009845897,0.00010739225,0.011965116,4.6391685e-7,0.00014374543,0.00015772857,0.009431697,0.0006924941,0.009592268,0.009263116,0.0069689606,0.01549733],[0.0008515702,0.00048757362,0.0021412952,0.014229859,0.005252996,0.010415156,0.014287308,0.012997905,0.013325559,0.015964549,0.0065351464,0.0007580837],[0.0045925723,0.018754967,0.0014369077,0.0010834769,0.0124404775,0.0018350423,0.0038135808,0.008261216,0.0034162104,0.0011589546,0.0007041311,0.010241941],[0.013932138,0.0075448267,0.0019522777,0.012203236,0.020048337,0.013246494,0.0015716385,0.0012640061,0.0072363764,0.013074266,0.008706542,0.0063366224],[0.015908567,0.014795145,0.015564123,0.008969006,0.018907672,0.013286521,0.008849657,0.0027541209,0.016721234,0.014685283,0.019363815,9.1746575e-9],[0.4593435,0.8983368,0.01365899,0.008506964,0.013812789,0.9293541,0.010772355,0.01860222,0.02852977,0.02761247,0.019766554,0.018179297],[0.009917188,0.016676152,0.01938403,0.014313027,0.024668086,0.012200025,0.030651039,0.011979866,0.009040098,0.9180728,0.03656692,0.012458795],[0.010785109,0.18082112,0.8458825,0.018389104,0.018577883,0.018624742,0.045821607,0.08584459,0.017446913,0.018828915,0.9134291,0.06278003],[0.016593171,0.4020053,0.017234534,0.043239776,0.0182269,0.014823633,0.14991932,0.013735405,0.030690031,0.031032033,0.06742787,0.024035063],[0.24678737,0.19003847,0.10590435,0.012400944,0.08340072,0.027829738,0.4409235,0.029761324,0.052328732,0.4667893,0.017830176,0.046301715],[0.3380286,0.50450027,0.040610585,0.14459799,0.06904957,0.016426167,0.30265534,0.4785508,0.052030724,0.01622346,0.15604296,0.25885916],[0.020020794,0.05508979,0.035944037,0.012775158,0.03888895,0.103544876,0.054535046,0.06993241,0.008403102,0.3095162,0.40937522,0.023624888]],"type":"heatmap","xaxis":"x","yaxis":"y","hovertemplate":"Head: %{x}<br>Layer: %{y}<br>color: %{z}<extra></extra>"}],                        {"template":{"data":{"histogram2dcontour":[{"type":"histogram2dcontour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"choropleth":[{"type":"choropleth","colorbar":{"outlinewidth":0,"ticks":""}}],"histogram2d":[{"type":"histogram2d","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmap":[{"type":"heatmap","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmapgl":[{"type":"heatmapgl","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"contourcarpet":[{"type":"contourcarpet","colorbar":{"outlinewidth":0,"ticks":""}}],"contour":[{"type":"contour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"surface":[{"type":"surface","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"mesh3d":[{"type":"mesh3d","colorbar":{"outlinewidth":0,"ticks":""}}],"scatter":[{"fillpattern":{"fillmode":"overlay","size":10,"solidity":0.2},"type":"scatter"}],"parcoords":[{"type":"parcoords","line":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolargl":[{"type":"scatterpolargl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"scattergeo":[{"type":"scattergeo","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolar":[{"type":"scatterpolar","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"scattergl":[{"type":"scattergl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatter3d":[{"type":"scatter3d","line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattermapbox":[{"type":"scattermapbox","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterternary":[{"type":"scatterternary","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattercarpet":[{"type":"scattercarpet","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"pie":[{"automargin":true,"type":"pie"}]},"layout":{"autotypenumbers":"strict","colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"hovermode":"closest","hoverlabel":{"align":"left"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"bgcolor":"#E5ECF6","angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"ternary":{"bgcolor":"#E5ECF6","aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"sequential":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"sequentialminus":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]]},"xaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"yaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"geo":{"bgcolor":"white","landcolor":"#E5ECF6","subunitcolor":"white","showland":true,"showlakes":true,"lakecolor":"white"},"title":{"x":0.05},"mapbox":{"style":"light"}}},"xaxis":{"anchor":"y","domain":[0.0,1.0],"scaleanchor":"y","constrain":"domain","title":{"text":"Head"}},"yaxis":{"anchor":"x","domain":[0.0,1.0],"autorange":"reversed","constrain":"domain","title":{"text":"Layer"}},"coloraxis":{"colorscale":[[0.0,"rgb(103,0,31)"],[0.1,"rgb(178,24,43)"],[0.2,"rgb(214,96,77)"],[0.3,"rgb(244,165,130)"],[0.4,"rgb(253,219,199)"],[0.5,"rgb(247,247,247)"],[0.6,"rgb(209,229,240)"],[0.7,"rgb(146,197,222)"],[0.8,"rgb(67,147,195)"],[0.9,"rgb(33,102,172)"],[1.0,"rgb(5,48,97)"]],"cmid":0.0},"title":{"text":"Induction Score by Head"}},                        {"responsive": true}                    ).then(function(){
                                     
         var gd = document.getElementById('0351729d-f7bd-4dff-8e0c-0159e788d6eb');
         var x = new MutationObserver(function (mutations, observer) {{
                 var display = window.getComputedStyle(gd).display;
                 if (!display || display === 'none') {{
                     console.log([gd, 'removed!']);
                     Plotly.purge(gd);
                     observer.disconnect();
                 }}
         }});

         // Listen for the removal of the full notebook cells
         var notebookContainer = gd.closest('#notebook-container');
         if (notebookContainer) {{
             x.observe(notebookContainer, {childList: true});
         }}

         // Listen for the clearing of the current output cell
         var outputEl = gd.closest('.output');
         if (outputEl) {{
             x.observe(outputEl, {childList: true});
         }}

                                 })                };                            </script>        </div>
         </body>
         </html>

.. container:: cell markdown

   Head 5 in Layer 5 scores extremely highly on this score, and we can
   feed in a shorter repeated random sequence, visualize the attention
   pattern for it and see this directly - including the "induction
   stripe" at ``seq_len-1`` tokens back.

   This time we put in a hook on the attention pattern activation to
   visualize the pattern of the relevant head.

.. container:: cell code

   .. code:: python

      induction_head_layer = 5
      induction_head_index = 5
      single_random_sequence = torch.randint(1000, 10000, (1, 20)).to(model.cfg.device)
      repeated_random_sequence = einops.repeat(single_random_sequence, "batch seq_len -> batch (2 seq_len)")
      def visualize_pattern_hook(
          pattern: TT["batch", "head_index", "dest_pos", "source_pos"],
          hook: HookPoint,
      ):
          display(
              cv.attention.attention_patterns(
                  tokens=model.to_str_tokens(repeated_random_sequence), 
                  attention=pattern[0, induction_head_index, :, :][None, :, :] # Add a dummy axis, as CircuitsVis expects 3D patterns.
              )
          )

      model.run_with_hooks(
          repeated_random_sequence, 
          return_type=None, 
          fwd_hooks=[(
              utils.get_act_name("pattern", induction_head_layer), 
              visualize_pattern_hook
          )]
      )

   .. container:: output stream stdout

      ::

         huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
         To disable this warning, you can either:
         	- Avoid using `tokenizers` before the fork if possible
         	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)

   .. container:: output display_data

      ::

         <circuitsvis.utils.render.RenderedHTML at 0x7f96783b37d0>

.. container:: cell markdown

   .. rubric:: Available Models
      :name: available-models

.. container:: cell markdown

   TransformerLens comes with over 40 open source models available, all
   of which can be loaded into a consistent(-ish) architecture by just
   changing the name in ``from_pretrained``. The open source models
   available are `documented
   here <https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=jHj79Pj58cgJKdq4t-ygK-4h>`__,
   and a set of interpretability friendly models I've trained are
   `documented
   here <https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=NCJ6zH_Okw_mUYAwGnMKsj2m>`__,
   including a set of toy language models (tiny one to four layer
   models) and a set of `SoLU
   models <https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=FZ5W6GGcy6OitPEaO733JLqf>`__
   up to GPT-2 Medium size (300M parameters). You can see `a table of
   the official alias and hyper-parameters of available models
   here <https://github.com/neelnanda-io/TransformerLens/blob/main/easy_transformer/model_properties_table.md>`__.

   **Note:** TransformerLens does not currently support multi-GPU models
   (which you want for models above eg 7B parameters), but this feature
   is coming soon!

.. container:: cell markdown

   Notably, this means that analysis can be near immediately re-run on a
   different model by just changing the name - to see this, let's load
   in DistilGPT-2 (a distilled version of GPT-2, with half as many
   layers) and copy the code from above to see the induction heads in
   that model.

.. container:: cell code

   .. code:: python

      distilgpt2 = HookedTransformer.from_pretrained("distilgpt2")
      # We make a tensor to store the induction score for each head. We put it on the model's device to avoid needing to move things between the GPU and CPU, which can be slow.
      distilgpt2_induction_score_store = torch.zeros((distilgpt2.cfg.n_layers, distilgpt2.cfg.n_heads), device=distilgpt2.cfg.device)
      def induction_score_hook(
          pattern: TT["batch", "head_index", "dest_pos", "source_pos"],
          hook: HookPoint,
      ):
          # We take the diagonal of attention paid from each destination position to source positions seq_len-1 tokens back
          # (This only has entries for tokens with index>=seq_len)
          induction_stripe = pattern.diagonal(dim1=-2, dim2=-1, offset=1-seq_len)
          # Get an average score per head
          induction_score = einops.reduce(induction_stripe, "batch head_index position -> head_index", "mean")
          # Store the result.
          distilgpt2_induction_score_store[hook.layer(), :] = induction_score

      # We make a boolean filter on activation names, that's true only on attention pattern names.
      pattern_hook_names_filter = lambda name: name.endswith("pattern")

      distilgpt2.run_with_hooks(
          repeated_tokens, 
          return_type=None, # For efficiency, we don't need to calculate the logits
          fwd_hooks=[(
              pattern_hook_names_filter,
              induction_score_hook
          )]
      )

      imshow(distilgpt2_induction_score_store, xaxis="Head", yaxis="Layer", title="Induction Score by Head in Distil GPT-2")

   .. container:: output stream stderr

      ::

         Using pad_token, but it is not set yet.

   .. container:: output stream stdout

      ::

         Loaded pretrained model distilgpt2 into HookedTransformer

   .. container:: output display_data

      .. raw:: html

         <html>
         <head><meta charset="utf-8" /></head>
         <body>
             <div>            <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG"></script><script type="text/javascript">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: "STIX-Web"}});}</script>                <script type="text/javascript">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>
                 <script src="https://cdn.plot.ly/plotly-2.16.1.min.js"></script>                <div id="98c37ee6-d1eb-4fd2-abb6-25fcea4806d8" class="plotly-graph-div" style="height:525px; width:100%;"></div>            <script type="text/javascript">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById("98c37ee6-d1eb-4fd2-abb6-25fcea4806d8")) {                    Plotly.newPlot(                        "98c37ee6-d1eb-4fd2-abb6-25fcea4806d8",                        [{"coloraxis":"coloraxis","name":"0","z":[[0.009829781,0.000295377,0.012661804,0.000002316661,0.00069736416,0.000026816326,0.00860497,0.0012993761,0.008594353,0.009532456,0.00913034,0.016025914],[0.00344231,0.017788095,0.0010160812,0.0001821529,0.013304929,0.0012890853,0.0038786372,0.014899377,0.0028749132,0.00015702132,0.003935385,0.014406526],[0.011763791,0.0076388153,0.013284326,0.0024857563,0.020266755,0.004270946,0.009777707,0.0016668368,0.018351581,0.013751813,0.022175496,1.0465541e-11],[0.00946849,0.23212358,0.86168015,0.01656808,0.01690789,0.015071683,0.02012734,0.19553702,0.015999738,0.01757678,0.93703943,0.51021636],[0.27425367,0.23033048,0.089379184,0.012144947,0.08233639,0.02498175,0.6287083,0.025814235,0.06939743,0.6472505,0.017307777,0.06159132],[0.02279091,0.08241454,0.056171473,0.013680244,0.03538631,0.18700893,0.08061893,0.10577389,0.007939984,0.46560174,0.17225139,0.031069668]],"type":"heatmap","xaxis":"x","yaxis":"y","hovertemplate":"Head: %{x}<br>Layer: %{y}<br>color: %{z}<extra></extra>"}],                        {"template":{"data":{"histogram2dcontour":[{"type":"histogram2dcontour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"choropleth":[{"type":"choropleth","colorbar":{"outlinewidth":0,"ticks":""}}],"histogram2d":[{"type":"histogram2d","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmap":[{"type":"heatmap","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmapgl":[{"type":"heatmapgl","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"contourcarpet":[{"type":"contourcarpet","colorbar":{"outlinewidth":0,"ticks":""}}],"contour":[{"type":"contour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"surface":[{"type":"surface","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"mesh3d":[{"type":"mesh3d","colorbar":{"outlinewidth":0,"ticks":""}}],"scatter":[{"fillpattern":{"fillmode":"overlay","size":10,"solidity":0.2},"type":"scatter"}],"parcoords":[{"type":"parcoords","line":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolargl":[{"type":"scatterpolargl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"scattergeo":[{"type":"scattergeo","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolar":[{"type":"scatterpolar","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"scattergl":[{"type":"scattergl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatter3d":[{"type":"scatter3d","line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattermapbox":[{"type":"scattermapbox","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterternary":[{"type":"scatterternary","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattercarpet":[{"type":"scattercarpet","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"pie":[{"automargin":true,"type":"pie"}]},"layout":{"autotypenumbers":"strict","colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"hovermode":"closest","hoverlabel":{"align":"left"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"bgcolor":"#E5ECF6","angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"ternary":{"bgcolor":"#E5ECF6","aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"sequential":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"sequentialminus":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]]},"xaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"yaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"geo":{"bgcolor":"white","landcolor":"#E5ECF6","subunitcolor":"white","showland":true,"showlakes":true,"lakecolor":"white"},"title":{"x":0.05},"mapbox":{"style":"light"}}},"xaxis":{"anchor":"y","domain":[0.0,1.0],"scaleanchor":"y","constrain":"domain","title":{"text":"Head"}},"yaxis":{"anchor":"x","domain":[0.0,1.0],"autorange":"reversed","constrain":"domain","title":{"text":"Layer"}},"coloraxis":{"colorscale":[[0.0,"rgb(103,0,31)"],[0.1,"rgb(178,24,43)"],[0.2,"rgb(214,96,77)"],[0.3,"rgb(244,165,130)"],[0.4,"rgb(253,219,199)"],[0.5,"rgb(247,247,247)"],[0.6,"rgb(209,229,240)"],[0.7,"rgb(146,197,222)"],[0.8,"rgb(67,147,195)"],[0.9,"rgb(33,102,172)"],[1.0,"rgb(5,48,97)"]],"cmid":0.0},"title":{"text":"Induction Score by Head in Distil GPT-2"}},                        {"responsive": true}                    ).then(function(){
                                     
         var gd = document.getElementById('98c37ee6-d1eb-4fd2-abb6-25fcea4806d8');
         var x = new MutationObserver(function (mutations, observer) {{
                 var display = window.getComputedStyle(gd).display;
                 if (!display || display === 'none') {{
                     console.log([gd, 'removed!']);
                     Plotly.purge(gd);
                     observer.disconnect();
                 }}
         }});

         // Listen for the removal of the full notebook cells
         var notebookContainer = gd.closest('#notebook-container');
         if (notebookContainer) {{
             x.observe(notebookContainer, {childList: true});
         }}

         // Listen for the clearing of the current output cell
         var outputEl = gd.closest('.output');
         if (outputEl) {{
             x.observe(outputEl, {childList: true});
         }}

                                 })                };                            </script>        </div>
         </body>
         </html>

.. container:: cell markdown

   .. rubric:: An overview of the important open source models in the
      library
      :name: an-overview-of-the-important-open-source-models-in-the-library

   -  **GPT-2** - the classic generative pre-trained models from OpenAI

      -  Sizes Small (85M), Medium (300M), Large (700M) and XL (1.5B).
      -  Trained on ~22B tokens of internet text. (`Open source
         replication <https://huggingface.co/datasets/openwebtext>`__)

   -  **GPT-Neo** - Eleuther's replication of GPT-2

      -  Sizes 125M, 1.3B, 2.7B
      -  Trained on 300B(ish?) tokens of `the
         Pile <https://pile.eleuther.ai/>`__ a large and diverse dataset
         including a bunch of code (and weird stuff)

   -  `OPT <https://ai.facebook.com/blog/democratizing-access-to-large-scale-language-models-with-opt-175b/>`__
      - Meta AI's series of open source models

      -  Trained on 180B tokens of diverse text.
      -  125M, 1.3B, 2.7B, 6.7B, 13B, 30B, 66B

   -  **GPT-J** - Eleuther's 6B parameter model, trained on the Pile
   -  **GPT-NeoX** - Eleuther's 20B parameter model, trained on the Pile
   -  **Stanford CRFM models** - a replication of GPT-2 Small and GPT-2
      Medium, trained on 5 different random seeds.

      -  Notably, 600 checkpoints were taken during training per model,
         and these are available in the library with eg
         ``HookedTransformer.from_pretrained("stanford-gpt2-small-a", checkpoint_index=265)``.

.. container:: cell markdown

   .. rubric:: An overview of some interpretability-friendly models I've
      trained and included
      :name: an-overview-of-some-interpretability-friendly-models-ive-trained-and-included

   (Feel free to `reach out <mailto:neelnanda27@gmail.com>`__ if you
   want more details on any of these models)

   Each of these models has about ~200 checkpoints taken during training
   that can also be loaded from TransformerLens, with the
   ``checkpoint_index`` argument to ``from_pretrained``.

   Note that all models are trained with a Beginning of Sequence token,
   and will likely break if given inputs without that!

   -  **Toy Models**: Inspired by `A Mathematical
      Framework <https://transformer-circuits.pub/2021/framework/index.html>`__,
      I've trained 12 tiny language models, of 1-4L and each of width
      512. I think that interpreting these is likely to be far more
      tractable than larger models, and both serve as good practice and
      will likely contain motifs and circuits that generalise to far
      larger models (like induction heads):

      -  Attention-Only models (ie without MLPs): attn-only-1l,
         attn-only-2l, attn-only-3l, attn-only-4l
      -  GELU models (ie with MLP, and the standard GELU activations):
         gelu-1l, gelu-2l, gelu-3l, gelu-4l
      -  SoLU models (ie with MLP, and `Anthropic's SoLU
         activation <https://transformer-circuits.pub/2022/solu/index.html>`__,
         designed to make MLP neurons more interpretable): solu-1l,
         solu-2l, solu-3l, solu-4l
      -  All models are trained on 22B tokens of data, 80% from C4 (web
         text) and 20% from Python Code
      -  Models of the same layer size were trained with the same weight
         initialization and data shuffle, to more directly compare the
         effect of different activation functions.

   -  **SoLU** models: A larger scan of models trained with `Anthropic's
      SoLU
      activation <https://transformer-circuits.pub/2022/solu/index.html>`__,
      in the hopes that it makes the MLP neuron interpretability easier.

      -  A scan up to GPT-2 Medium size, trained on 30B tokens of the
         same data as toy models, 80% from C4 and 20% from Python code.

         -  solu-6l (40M), solu-8l (100M), solu-10l (200M), solu-12l
            (340M)

      -  An older scan up to GPT-2 Medium size, trained on 15B tokens of
         `the Pile <https://pile.eleuther.ai/>`__

         -  solu-1l-pile (13M), solu-2l-pile (13M), solu-4l-pile (13M),
            solu-6l-pile (40M), solu-8l-pile (100M), solu-10l-pile
            (200M), solu-12l-pile (340M)

.. container:: cell markdown

   .. rubric:: Other Resources:
      :name: other-resources

   -  `Concrete Open Problems in Mechanistic
      Interpretability <https://docs.google.com/document/d/1WONBzNqfKIxERejrrPlQMyKqg7jSFW92x5UMXNrMdPo/edit>`__,
      a doc I wrote giving a long list of open problems in mechanistic
      interpretability, and thoughts on how to get started on trying to
      work on them.

      -  There's a lot of low-hanging fruit in the field, and I expect
         that many people reading this could use TransformerLens to
         usefully make progress on some of these!

   -  Other demos:

      -  `Exploratory Analysis
         Demo <https://colab.research.google.com/github/neelnanda-io/TransformerLens/blob/main/Exploratory_Analysis_Demo.ipynb>`__,
         a demonstration of my standard toolkit for how to use
         TransformerLens to explore a mysterious behaviour in a language
         model.
      -  `Interpretability in the
         Wild <https://github.com/redwoodresearch/Easy-Transformer>`__ a
         codebase from Arthur Conmy and Alex Variengien at Redwood
         research using this library to do a detailed and rigorous
         reverse engineering of the Indirect Object Identification
         circuit, to accompany their paper

         -  Note - this was based on an earlier version of this library,
            called EasyTransformer. It's pretty similar, but several
            breaking changes have been made since.

      -  A `recorded
         walkthrough <https://www.youtube.com/watch?v=yo4QvDn-vsU>`__ of
         me doing research with TransformerLens on whether a tiny model
         can re-derive positional information, with `an accompanying
         Colab <https://colab.research.google.com/github/neelnanda-io/TransformerLens/blob/main/No_Position_Experiment.ipynb>`__

   -  `Neuroscope <https://neuroscope.io>`__, a website showing the text
      in the dataset that most activates each neuron in some selected
      models. Good to explore to get a sense for what kind of features
      the model tends to represent, and as a "wiki" to get some info

      -  A tutorial on how to make an `Interactive
         Neuroscope <https://github.com/neelnanda-io/TransformerLens/blob/main/Hacky-Interactive-Lexoscope.ipynb>`__,
         where you type in text and see the neuron activations over the
         text update live.

.. container:: cell markdown

   .. rubric:: Transformer architecture
      :name: transformer-architecture

   HookedTransformer is a somewhat adapted GPT-2 architecture, but is
   computationally identical. The most significant changes are to the
   internal structure of the attention heads:

   -  The weights (W_K, W_Q, W_V) mapping the residual stream to
      queries, keys and values are 3 separate matrices, rather than big
      concatenated one.
   -  The weight matrices (W_K, W_Q, W_V, W_O) and activations (keys,
      queries, values, z (values mixed by attention pattern)) have
      separate head_index and d_head axes, rather than flattening them
      into one big axis.

      -  The activations all have shape
         ``[batch, position, head_index, d_head]``
      -  W_K, W_Q, W_V have shape ``[head_index, d_head, d_model]`` and
         W_O has shape ``[head_index, d_model, d_head]``

   The actual code is a bit of a mess, as there's a variety of Boolean
   flags to make it consistent with the various different model families
   in TransformerLens - to understand it and the internal structure, I
   instead recommend reading the code in
   `CleanTransformerDemo <https://colab.research.google.com/github/neelnanda-io/TransformerLens/blob/clean-transformer-demo/Clean_Transformer_Demo.ipynb>`__

.. container:: cell markdown

   .. rubric:: Parameter Names
      :name: parameter-names

   Here is a list of the parameters and shapes in the model. By
   convention, all weight matrices multiply on the right (ie
   ``new_activation = old_activation @ weights + bias``).

   Reminder of the key hyper-params:

   -  ``n_layers``: 12. The number of transformer blocks in the model (a
      block contains an attention layer and an MLP layer)
   -  ``n_heads``: 12. The number of attention heads per attention layer
   -  ``d_model``: 768. The residual stream width.
   -  ``d_head``: 64. The internal dimension of an attention head
      activation.
   -  ``d_mlp``: 3072. The internal dimension of the MLP layers (ie the
      number of neurons).
   -  ``d_vocab``: 50267. The number of tokens in the vocabulary.
   -  ``n_ctx``: 1024. The maximum number of tokens in an input prompt.

.. container:: cell markdown

   **Transformer Block parameters:** Replace 0 with the relevant layer
   index.

.. container:: cell code

   .. code:: python

      for name, param in model.named_parameters():
          if name.startswith("blocks.0."):
              print(name, param.shape)

   .. container:: output stream stdout

      ::

         blocks.0.attn.W_Q torch.Size([12, 768, 64])
         blocks.0.attn.W_K torch.Size([12, 768, 64])
         blocks.0.attn.W_V torch.Size([12, 768, 64])
         blocks.0.attn.W_O torch.Size([12, 64, 768])
         blocks.0.attn.b_Q torch.Size([12, 64])
         blocks.0.attn.b_K torch.Size([12, 64])
         blocks.0.attn.b_V torch.Size([12, 64])
         blocks.0.attn.b_O torch.Size([768])
         blocks.0.mlp.W_in torch.Size([768, 3072])
         blocks.0.mlp.b_in torch.Size([3072])
         blocks.0.mlp.W_out torch.Size([3072, 768])
         blocks.0.mlp.b_out torch.Size([768])

.. container:: cell markdown

   **Embedding & Unembedding parameters:**

.. container:: cell code

   .. code:: python

      for name, param in model.named_parameters():
          if not name.startswith("blocks"):
              print(name, param.shape)

   .. container:: output stream stdout

      ::

         embed.W_E torch.Size([50257, 768])
         pos_embed.W_pos torch.Size([1024, 768])
         unembed.W_U torch.Size([768, 50257])
         unembed.b_U torch.Size([50257])

.. container:: cell markdown

   .. rubric:: Activation + Hook Names
      :name: activation--hook-names

   Lets get out a list of the activation/hook names in the model and
   their shapes. In practice, I recommend using the
   ``utils.get_act_name`` function to get the names, but this is a
   useful fallback, and necessary to eg write a name filter function.

   Let's do this by entering in a short, 10 token prompt, and add a hook
   function to each activations to print its name and shape. To avoid
   spam, let's just add this to activations in the first block or not in
   a block.

   Note 1: Each LayerNorm has a hook for the scale factor (ie the
   standard deviation of the input activations for each token position &
   batch element) and for the normalized output (ie the input activation
   with mean 0 and standard deviation 1, but *before* applying scaling
   or translating with learned weights). LayerNorm is applied every time
   a layer reads from the residual stream: ``ln1`` is the LayerNorm
   before the attention layer in a block, ``ln2`` the one before the MLP
   layer, and ``ln_final`` is the LayerNorm before the unembed.

   Note 2: *Every* activation apart from the attention pattern and
   attention scores has shape beginning with ``[batch, position]``. The
   attention pattern and scores have shape
   ``[batch, head_index, dest_position, source_position]`` (the numbers
   are the same, unless we're using caching).

.. container:: cell code

   .. code:: python

      test_prompt = "The quick brown fox jumped over the lazy dog"
      print("Num tokens:", len(model.to_tokens(test_prompt)))

      def print_name_shape_hook_function(activation, hook):
          print(hook.name, activation.shape)

      not_in_late_block_filter = lambda name: name.startswith("blocks.0.") or not name.startswith("blocks")

      model.run_with_hooks(
          test_prompt,
          return_type=None,
          fwd_hooks=[(not_in_late_block_filter, print_name_shape_hook_function)],
      )

   .. container:: output stream stdout

      ::

         Num tokens: 1
         hook_embed torch.Size([1, 10, 768])
         hook_pos_embed torch.Size([1, 10, 768])
         blocks.0.hook_resid_pre torch.Size([1, 10, 768])
         blocks.0.ln1.hook_scale torch.Size([1, 10, 1])
         blocks.0.ln1.hook_normalized torch.Size([1, 10, 768])
         blocks.0.attn.hook_q torch.Size([1, 10, 12, 64])
         blocks.0.attn.hook_k torch.Size([1, 10, 12, 64])
         blocks.0.attn.hook_v torch.Size([1, 10, 12, 64])
         blocks.0.attn.hook_attn_scores torch.Size([1, 12, 10, 10])
         blocks.0.attn.hook_pattern torch.Size([1, 12, 10, 10])
         blocks.0.attn.hook_z torch.Size([1, 10, 12, 64])
         blocks.0.hook_attn_out torch.Size([1, 10, 768])
         blocks.0.hook_resid_mid torch.Size([1, 10, 768])
         blocks.0.ln2.hook_scale torch.Size([1, 10, 1])
         blocks.0.ln2.hook_normalized torch.Size([1, 10, 768])
         blocks.0.mlp.hook_pre torch.Size([1, 10, 3072])
         blocks.0.mlp.hook_post torch.Size([1, 10, 3072])
         blocks.0.hook_mlp_out torch.Size([1, 10, 768])
         blocks.0.hook_resid_post torch.Size([1, 10, 768])
         ln_final.hook_scale torch.Size([1, 10, 1])
         ln_final.hook_normalized torch.Size([1, 10, 768])

.. container:: cell markdown

   .. rubric:: Folding LayerNorm (For the Curious)
      :name: folding-layernorm-for-the-curious

.. container:: cell markdown

   (For the curious - this is an important technical detail that's worth
   understanding, especially if you have preconceptions about how
   transformers work, but not necessary to use TransformerLens)

   LayerNorm is a normalization technique used by transformers,
   analogous to BatchNorm but more friendly to massive parallelisation.
   No one *really* knows why it works, but it seems to improve model
   numerical stability. Unlike BatchNorm, LayerNorm actually changes the
   functional form of the model, which makes it a massive pain for
   interpretability!

   Folding LayerNorm is a technique to make it lower overhead to deal
   with, and the flags ``center_writing_weights`` and ``fold_ln`` in
   ``HookedTransformer.from_pretrained`` apply this automatically (they
   default to True). These simplify the internal structure without
   changing the weights.

   Intuitively, LayerNorm acts on each residual stream vector (ie for
   each batch element and token position) independently, sets their mean
   to 0 (centering) and standard deviation to 1 (normalizing) (*across*
   the residual stream dimension - very weird!), and then applies a
   learned elementwise scaling and translation to each vector.

   Mathematically, centering is a linear map, normalizing is *not* a
   linear map, and scaling and translation are linear maps.

   -  **Centering:** LayerNorm is applied every time a layer reads from
      the residual stream, so the mean of any residual stream vector can
      never matter - ``center_writing_weights`` set every weight matrix
      writing to the residual to have zero mean.
   -  **Normalizing:** Normalizing is not a linear map, and cannot be
      factored out. The ``hook_scale`` hook point lets you access and
      control for this.
   -  **Scaling and Translation:** Scaling and translation are linear
      maps, and are always followed by another linear map. The
      composition of two linear maps is another linear map, so we can
      *fold* the scaling and translation weights into the weights of the
      subsequent layer, and simplify things without changing the
      underlying computation.

   `See the docs for more
   details <https://github.com/neelnanda-io/TransformerLens/blob/main/further_comments.md#what-is-layernorm-folding-fold_ln>`__

.. container:: cell markdown

   A fun consequence of LayerNorm folding is that it creates a bias
   across the unembed, a ``d_vocab`` length vector that is added to the
   output logits - GPT-2 is not trained with this, but it *is* trained
   with a final LayerNorm that contains a bias.

   Turns out, this LayerNorm bias learns structure of the data that we
   can only see after folding! In particular, it essentially learns
   **unigram statistics** - rare tokens get suppressed, common tokens
   get boosted, by pretty dramatic degrees! Let's list the top and
   bottom 20 - at the top we see common punctuation and words like "
   the" and " and", at the bottom we see weird-ass tokens like "
   RandomRedditor":

.. container:: cell code

   .. code:: python

      unembed_bias = model.unembed.b_U
      bias_values, bias_indices = unembed_bias.sort(descending=True)

.. container:: cell code

   .. code:: python

      top_k = 20
      print(f"Top {top_k} values")
      for i in range(top_k):
          print(f"{bias_values[i].item():.2f} {repr(model.to_string(bias_indices[i]))}")

      print("...")
      print(f"Bottom {top_k} values")
      for i in range(top_k, 0, -1):
          print(f"{bias_values[-i].item():.2f} {repr(model.to_string(bias_indices[-i]))}")

   .. container:: output stream stdout

      ::

         Top 20 values
         7.03 ','
         6.98 ' the'
         6.68 ' and'
         6.49 '.'
         6.48 '\n'
         6.47 ' a'
         6.41 ' in'
         6.25 ' to'
         6.16 ' of'
         6.04 '-'
         6.03 ' ('
         5.88 ' "'
         5.80 ' for'
         5.72 ' that'
         5.64 ' on'
         5.59 ' is'
         5.52 ' as'
         5.49 ' at'
         5.45 ' with'
         5.44 ' or'
         ...
         Bottom 20 values
         -3.82 ' サーティ'
         -3.83 '\x18'
         -3.83 '\x14'
         -3.83 ' RandomRedditor'
         -3.83 '龍�'
         -3.83 '�'
         -3.83 '\x1b'
         -3.83 '�'
         -3.83 '\x05'
         -3.83 '\x00'
         -3.83 '\x06'
         -3.83 '\x07'
         -3.83 '\x0c'
         -3.83 '\x02'
         -3.83 'oreAndOnline'
         -3.84 '\x11'
         -3.84 '�'
         -3.84 '\x10'
         -3.84 '�'
         -3.84 '�'

.. container:: cell markdown

   This can have real consequences for interpretability - for example,
   this bias favours " John" over " Mary" by about 1.2, about 1/3 of the
   effect size of the Indirect Object Identification Circuit! All other
   things being the same, this makes the John token 3.6x times more
   likely than the Mary token.

.. container:: cell code

   .. code:: python

      john_bias = model.unembed.b_U[model.to_single_token(' John')]
      mary_bias = model.unembed.b_U[model.to_single_token(' Mary')]

      print(f"John bias: {john_bias.item():.4f}")
      print(f"Mary bias: {mary_bias.item():.4f}")
      print(f"Prob ratio bias: {torch.exp(john_bias - mary_bias).item():.4f}x")

   .. container:: output stream stdout

      ::

         John bias: 2.8995
         Mary bias: 1.6034
         Prob ratio bias: 3.6550x

.. container:: cell markdown

   .. rubric:: Features
      :name: features

   An overview of some other important features of the library. I
   recommend checking out the `Exploratory Analysis
   Demo <https://colab.research.google.com/github/neelnanda-io/Easy-Transformer/blob/main/Exploratory_Analysis_Demo.ipynb>`__
   for some other important features not mentioned here, and for a demo
   of what using the library in practice looks like.

.. container:: cell markdown

   .. rubric:: Dealing with tokens
      :name: dealing-with-tokens

   **Tokenization** is one of the most annoying features of studying
   language models. We want language models to be able to take in
   arbitrary text as input, but the transformer architecture needs the
   inputs to be elements of a fixed, finite vocabulary. The solution to
   this is **tokens**, a fixed vocabulary of "sub-words", that any
   natural language can be broken down into with a **tokenizer**. This
   is invertible, and we can recover the original text, called
   **de-tokenization**.

   TransformerLens comes with a range of utility functions to deal with
   tokenization. Different models can have different tokenizers, so
   these are all methods on the model.

   get_token_position, to_tokens, to_string, to_str_tokens, prepend_bos,
   to_single_token

.. container:: cell markdown

   The first thing you need to figure out is *how* things are tokenized.
   ``model.to_str_tokens`` splits a string into the tokens *as a list of
   substrings*, and so lets you explore what the text looks like. To
   demonstrate this, let's use it on this paragraph.

   Some observations - there are a lot of arbitrary-ish details in here!

   -  The tokenizer splits on spaces, so no token contains two words.
   -  Tokens include the preceding space, and whether the first token is
      a capital letter. ``how`` and ``how`` are different tokens!
   -  Common words are single tokens, even if fairly long
      (``paragraph``) while uncommon words are split into multiple
      tokens (``token|ized``).
   -  Tokens *mostly* split on punctuation characters (eg ``*`` and
      ``.``), but eg ``'s`` is a single token.

.. container:: cell code

   .. code:: python

      example_text = "The first thing you need to figure out is *how* things are tokenized. `model.to_str_tokens` splits a string into the tokens *as a list of substrings*, and so lets you explore what the text looks like. To demonstrate this, let's use it on this paragraph."
      example_text_str_tokens = model.to_str_tokens(example_text)
      print(example_text_str_tokens)

   .. container:: output stream stdout

      ::

         ['<|endoftext|>', 'The', ' first', ' thing', ' you', ' need', ' to', ' figure', ' out', ' is', ' *', 'how', '*', ' things', ' are', ' token', 'ized', '.', ' `', 'model', '.', 'to', '_', 'str', '_', 't', 'ok', 'ens', '`', ' splits', ' a', ' string', ' into', ' the', ' tokens', ' *', 'as', ' a', ' list', ' of', ' sub', 'strings', '*,', ' and', ' so', ' lets', ' you', ' explore', ' what', ' the', ' text', ' looks', ' like', '.', ' To', ' demonstrate', ' this', ',', ' let', "'s", ' use', ' it', ' on', ' this', ' paragraph', '.']

.. container:: cell markdown

   The transformer needs to take in a sequence of integers, not strings,
   so we need to convert these tokens into integers. ``model.to_tokens``
   does this, and returns a tensor of integers on the model's device
   (shape ``[batch, position]``). It maps a string to a batch of size 1.

.. container:: cell code

   .. code:: python

      example_text_tokens = model.to_tokens(example_text)
      print(example_text_tokens)

   .. container:: output stream stdout

      ::

         tensor([[50256,   464,   717,  1517,   345,   761,   284,  3785,   503,   318,
                   1635,  4919,     9,  1243,   389, 11241,  1143,    13,  4600, 19849,
                     13,  1462,    62,  2536,    62,    83,   482,   641,    63, 30778,
                    257,  4731,   656,   262, 16326,  1635,   292,   257,  1351,   286,
                    850, 37336, 25666,   290,   523,  8781,   345,  7301,   644,   262,
                   2420,  3073,   588,    13,  1675, 10176,   428,    11,  1309,   338,
                    779,   340,   319,   428,  7322,    13]], device='cuda:0')

.. container:: cell markdown

   ``to_tokens`` can also take in a list of strings, and return a batch
   of size ``len(strings)``. If the strings are different numbers of
   tokens, it adds a PAD token to the end of the shorter strings to make
   them the same length.

   (Note: In GPT-2, 50256 signifies both the beginning of sequence, end
   of sequence and padding token - see the ``prepend_bos`` section for
   details)

.. container:: cell code

   .. code:: python

      example_multi_text = ["The cat sat on the mat.", "The cat sat on the mat really hard."]
      example_multi_text_tokens = model.to_tokens(example_multi_text)
      print(example_multi_text_tokens)

   .. container:: output stream stdout

      ::

         tensor([[50256,   464,  3797,  3332,   319,   262,  2603,    13, 50256, 50256],
                 [50256,   464,  3797,  3332,   319,   262,  2603,  1107,  1327,    13]],
                device='cuda:0')

.. container:: cell markdown

   ``model.to_single_token`` is a convenience function that takes in a
   string corresponding to a *single* token and returns the
   corresponding integer. This is useful for eg looking up the logit
   corresponding to a single token.

   For example, let's input ``The cat sat on the mat.`` to GPT-2, and
   look at the log prob predicting that the next token is ``The``.

   .. raw:: html

      <details><summary>Technical notes</summary>

      Note that if we input a string to the model, it's implicitly converted to a string with `to_tokens`. 

      Note further that the log probs have shape `[batch, position, d_vocab]==[1, 8, 50257]`, with a vector of log probs predicting the next token for *every* token position. GPT-2 uses causal attention which means heads can only look backwards (equivalently, information can only move forwards in the model.), so the log probs at position k are only a function of the first k tokens, and it can't just cheat and look at the k+1 th token. This structure lets it generate text more efficiently, and lets it treat every *token* as a training example, rather than every *sequence*.
      </details>

.. container:: cell code

   .. code:: python

      cat_text = "The cat sat on the mat."
      cat_logits = model(cat_text)
      cat_probs = cat_logits.softmax(dim=-1)
      print(f"Probability tensor shape [batch, position, d_vocab] == {cat_probs.shape}")

      capital_the_token_index = model.to_single_token(" The")
      print(f"| The| probability: {cat_probs[0, -1, capital_the_token_index].item():.2%}")

   .. container:: output stream stdout

      ::

         Probability tensor shape [batch, position, d_vocab] == torch.Size([1, 8, 50257])
         | The| probability: 11.98%

.. container:: cell markdown

   ``model.to_string`` is the inverse of ``to_tokens`` and maps a tensor
   of integers to a string or list of strings. It also works on integers
   and lists of integers.

   For example, let's look up token 256 (due to technical details of
   tokenization, this will be the most common pair of ASCII
   characters!), and also verify that our tokens above map back to a
   string.

.. container:: cell code

   .. code:: python

      print(f"Token 256 - the most common pair of ASCII characters: |{model.to_string(256)}|")
      # Squeeze means to remove dimensions of length 1. 
      # Here, that removes the dummy batch dimension so it's a rank 1 tensor and returns a string
      # Rank 2 tensors map to a list of strings
      print(f"De-Tokenizing the example tokens: {model.to_string(example_text_tokens.squeeze())}")

   .. container:: output stream stdout

      ::

         Token 256 - the most common pair of ASCII characters: | t|
         De-Tokenizing the example tokens: <|endoftext|>The first thing you need to figure out is *how* things are tokenized. `model.to_str_tokens` splits a string into the tokens *as a list of substrings*, and so lets you explore what the text looks like. To demonstrate this, let's use it on this paragraph.

.. container:: cell markdown

   A related annoyance of tokenization is that it's hard to figure out
   how many tokens a string will break into.
   ``model.get_token_position(single_token, tokens)`` returns the
   position of ``single_token`` in ``tokens``. ``tokens`` can be either
   a string or a tensor of tokens.

   Note that position is zero-indexed, it's two (ie third) because
   there's a beginning of sequence token automatically prepended (see
   the next section for details)

.. container:: cell code

   .. code:: python

      print("With BOS:", model.get_token_position(" cat", "The cat sat on the mat"))
      print("Without BOS:", model.get_token_position(" cat", "The cat sat on the mat", prepend_bos=False))

   .. container:: output stream stdout

      ::

         With BOS: 2
         Without BOS: 1

.. container:: cell markdown

   If there are multiple copies of the token, we can set
   ``mode="first"`` to find the first occurence's position and
   ``mode="last"`` to find the last

.. container:: cell code

   .. code:: python

      print("First occurence", model.get_token_position(
          " cat", 
          "The cat sat on the mat. The mat sat on the cat.", 
          mode="first"))
      print("Final occurence", model.get_token_position(
          " cat", 
          "The cat sat on the mat. The mat sat on the cat.", 
          mode="last"))

   .. container:: output stream stdout

      ::

         First occurence 2
         Final occurence 13

.. container:: cell markdown

   In general, tokenization is a pain, and full of gotchas. I highly
   recommend just playing around with different inputs and their
   tokenization and getting a feel for it. As another "fun" example,
   let's look at the tokenization of arithmetic expressions - tokens do
   *not* contain consistent numbers of digits. (This makes it even more
   impressive that GPT-3 can do arithmetic!)

.. container:: cell code

   .. code:: python

      print(model.to_str_tokens("2342+2017=21445"))
      print(model.to_str_tokens("1000+1000000=999999"))

   .. container:: output stream stdout

      ::

         ['<|endoftext|>', '23', '42', '+', '2017', '=', '214', '45']
         ['<|endoftext|>', '1000', '+', '1', '000000', '=', '9999', '99']

.. container:: cell markdown

   I also *highly* recommend investigating prompts with easy
   tokenization when starting out - ideally key words should form a
   single token, be in the same position in different prompts, have the
   same total length, etc. Eg study Indirect Object Identification with
   common English names like ``Tim`` rather than ``Ne|el``. Transformers
   need to spend some parameters in early layers converting multi-token
   words to a single feature, and then de-converting this in the late
   layers, and unless this is what you're explicitly investigating, this
   will make the behaviour you're investigating be messier.

.. container:: cell markdown

   .. rubric:: Gotcha: ``prepend_bos``
      :name: gotcha-prepend_bos

   Key Takeaway: **If you get weird off-by-one errors, check whether
   there's an unexpected ``prepend_bos``!**

.. container:: cell markdown

   A weirdness you may have noticed in the above is that ``to_tokens``
   and ``to_str_tokens`` added a weird ``<|endoftext|>`` to the start of
   each prompt. TransformerLens does this by default, and it can easily
   trip up new users. Notably, **this includes ``model.forward``**
   (which is what's implicitly used when you do eg
   ``model("Hello World")``). This is called a **Beginning of Sequence
   (BOS)** token, and it's a special token used to mark the beginning of
   the sequence. Confusingly, in GPT-2, the End of Sequence (EOS),
   Beginning of Sequence (BOS) and Padding (PAD) tokens are all the
   same, ``<|endoftext|>`` with index ``50256``.

   You can disable this behaviour by setting the flag
   ``prepend_bos=False`` in ``to_tokens``, ``to_str_tokens``,
   ``model.forward`` and any other function that converts strings to
   multi-token tensors.

   **Gotcha:** You only want to do this at the *start* of a prompt. If
   you, eg, want to input a question followed by an answer, and want to
   tokenize these separately, you do *not* want to prepend_bos on the
   answer.

.. container:: cell code

   .. code:: python

      print("Logits shape by default (with BOS)", model("Hello World").shape)
      print("Logits shape with BOS", model("Hello World", prepend_bos=True).shape)
      print("Logits shape without BOS - only 2 positions!", model("Hello World", prepend_bos=False).shape)

   .. container:: output stream stdout

      ::

         Logits shape by default (with BOS) torch.Size([1, 3, 50257])
         Logits shape with BOS torch.Size([1, 3, 50257])
         Logits shape without BOS - only 2 positions! torch.Size([1, 2, 50257])

.. container:: cell markdown

   ``prepend_bos`` is a bit of a hack, and I've gone back and forth on
   what the correct default here is. The reason I do this is that
   transformers tend to treat the first token weirdly - this doesn't
   really matter in training (where all inputs are >1000 tokens), but
   this can be a big issue when investigating short prompts! The reason
   for this is that attention patterns are a probability distribution
   and so need to add up to one, so to simulate being "off" they
   normally look at the first token. Giving them a BOS token lets the
   heads rest by looking at that, preserving the information in the
   first "real" token.

   Further, *some* models are trained to need a BOS token (OPT and my
   interpretability-friendly models are, GPT-2 and GPT-Neo are not). But
   despite GPT-2 not being trained with this, empirically it seems to
   make interpretability easier.

   For example, the model can get much worse at Indirect Object
   Identification without a BOS (and with a name as the first token):

.. container:: cell code

   .. code:: python

      ioi_logits_with_bos = model("Claire and Mary went to the shops, then Mary gave a bottle of milk to", prepend_bos=True)
      mary_logit_with_bos = ioi_logits_with_bos[0, -1, model.to_single_token(" Mary")].item()
      claire_logit_with_bos = ioi_logits_with_bos[0, -1, model.to_single_token(" Claire")].item()
      print(f"Logit difference with BOS: {(claire_logit_with_bos - mary_logit_with_bos):.3f}")

      ioi_logits_without_bos = model("Claire and Mary went to the shops, then Mary gave a bottle of milk to", prepend_bos=False)
      mary_logit_without_bos = ioi_logits_without_bos[0, -1, model.to_single_token(" Mary")].item()
      claire_logit_without_bos = ioi_logits_without_bos[0, -1, model.to_single_token(" Claire")].item()
      print(f"Logit difference without BOS: {(claire_logit_without_bos - mary_logit_without_bos):.3f}")

   .. container:: output stream stdout

      ::

         Logit difference with BOS: 6.754
         Logit difference without BOS: 2.782

.. container:: cell markdown

   Though, note that this also illustrates another gotcha - when
   ``Claire`` is at the start of a sentence (no preceding space), it's
   actually *two* tokens, not one, which probably confuses the relevant
   circuit. (Note - in this test we put ``prepend_bos=False``, because
   we want to analyse the tokenization of a specific string, not to give
   an input to the model!)

.. container:: cell code

   .. code:: python

      print(f"| Claire| -> {model.to_str_tokens(' Claire', prepend_bos=False)}")
      print(f"|Claire| -> {model.to_str_tokens('Claire', prepend_bos=False)}")

   .. container:: output stream stdout

      ::

         | Claire| -> [' Claire']
         |Claire| -> ['Cl', 'aire']

.. container:: cell markdown

   .. rubric:: Factored Matrix Class
      :name: factored-matrix-class

   In transformer interpretability, we often need to analyse low rank
   factorized matrices - a matrix :math:`M = AB`, where M is
   ``[large, large]``, but A is ``[large, small]`` and B is
   ``[small, large]``. This is a common structure in transformers, and
   the ``FactoredMatrix`` class is a convenient way to work with these.
   It implements efficient algorithms for various operations on these,
   such as computing the trace, eigenvalues, Frobenius norm, singular
   value decomposition, and products with other matrices. It can
   (approximately) act as a drop-in replacement for the original matrix,
   and supports leading batch dimensions to the factored matrix.

   .. raw:: html

      <details><summary>Why are low-rank factorized matrices useful for transformer interpretability?</summary>

      As argued in [A Mathematical Framework](https://transformer-circuits.pub/2021/framework/index.html), an unexpected fact about transformer attention heads is that rather than being best understood as keys, queries and values (and the requisite weight matrices), they're actually best understood as two low rank factorized matrices. 
      * **Where to move information from:** $W_QK = W_Q W_K^T$, used for determining the attention pattern - what source positions to move information from and what destination positions to move them to.
          * Intuitively, residual stream -> query and residual stream -> key are linear maps, *and* `attention_score = query @ key.T` is a linear map, so the whole thing can be factored into one big bilinear form `residual @ W_QK @ residual.T`
      * **What information to move:** $W_OV = W_V W_O$, used to determine what information to copy from the source position to the destination position (weighted by the attention pattern weight from that destination to that source). 
          * Intuitively, the residual stream is a `[position, d_model]` tensor (ignoring batch). The attention pattern acts on the *position* dimension (where to move information from and to) and the value and output weights act on the *d_model* dimension - ie *what* information is contained at that source position. So we can factor it all into `attention_pattern @ residual @ W_V @ W_O`, and so only need to care about `W_OV = W_V @ W_O`
      * Note - the internal head dimension is smaller than the residual stream dimension, so the factorization is low rank. (here, `d_model=768` and `d_head=64`)
      </details>

.. container:: cell markdown

   .. rubric:: Basic Examples
      :name: basic-examples

.. container:: cell markdown

   We can use the basic class directly - let's make a factored matrix
   directly and look at the basic operations:

.. container:: cell code

   .. code:: python

      A = torch.randn(5, 2)
      B = torch.randn(2, 5)
      AB = A @ B
      AB_factor = FactoredMatrix(A, B)
      print("Norms:")
      print(AB.norm())
      print(AB_factor.norm())

      print(f"Right dimension: {AB_factor.rdim}, Left dimension: {AB_factor.ldim}, Hidden dimension: {AB_factor.mdim}")

   .. container:: output stream stdout

      ::

         Norms:
         tensor(5.7439)
         tensor(5.7439)
         Right dimension: 5, Left dimension: 5, Hidden dimension: 2

.. container:: cell markdown

   We can also look at the eigenvalues and singular values of the
   matrix. Note that, because the matrix is rank 2 but 5 by 5, the final
   3 eigenvalues and singular values are zero - the factored class omits
   the zeros.

.. container:: cell code

   .. code:: python

      print("Eigenvalues:")
      print(torch.linalg.eig(AB).eigenvalues)
      print(AB_factor.eigenvalues)
      print()
      print("Singular Values:")
      print(torch.linalg.svd(AB).S)
      print(AB_factor.S)

   .. container:: output stream stdout

      ::

         Eigenvalues:
         tensor([-1.6199e+00+0.0000e+00j,  1.0446e+00+0.0000e+00j,
                  3.7190e-08+0.0000e+00j, -6.5998e-08+1.2859e-07j,
                 -6.5998e-08-1.2859e-07j])
         tensor([-1.6199+0.j,  1.0446+0.j])

         Singular Values:
         tensor([5.4702e+00, 1.7519e+00, 3.4613e-07, 1.0601e-07, 3.8823e-09])
         tensor([5.4702, 1.7519])

.. container:: cell markdown

   We can multiply with other matrices - it automatically chooses the
   smallest possible dimension to factor along (here it's 2, rather than
   5)

.. container:: cell code

   .. code:: python

      C = torch.randn(5, 300)
      ABC = AB @ C
      ABC_factor = AB_factor @ C
      print("Unfactored:", ABC.shape, ABC.norm())
      print("Factored:", ABC_factor.shape, ABC_factor.norm())
      print(f"Right dimension: {ABC_factor.rdim}, Left dimension: {ABC_factor.ldim}, Hidden dimension: {ABC_factor.mdim}")

   .. container:: output stream stdout

      ::

         Unfactored: torch.Size([5, 300]) tensor(99.9906)
         Factored: torch.Size([5, 300]) tensor(99.9906)
         Right dimension: 300, Left dimension: 5, Hidden dimension: 2

.. container:: cell markdown

   If we want to collapse this back to an unfactored matrix, we can use
   the AB property to get the product:

.. container:: cell code

   .. code:: python

      AB_unfactored = AB_factor.AB
      print(torch.isclose(AB_unfactored, AB).all())

   .. container:: output stream stdout

      ::

         tensor(True)

.. container:: cell markdown

   .. rubric:: Medium Example: Eigenvalue Copying Scores
      :name: medium-example-eigenvalue-copying-scores

   (This is a more involved example of how to use the factored matrix
   class, skip it if you aren't following)

   For a more involved example, let's look at the eigenvalue copying
   score from `A Mathematical
   Framework <https://transformer-circuits.pub/2021/framework/index.html>`__
   of the OV circuit for various heads. The OV Circuit for a head (the
   factorised matrix :math:`W_OV = W_V W_O`) is a linear map that
   determines what information is moved from the source position to the
   destination position. Because this is low rank, it can be thought of
   as *reading in* some low rank subspace of the source residual stream
   and *writing to* some low rank subspace of the destination residual
   stream (with maybe some processing happening in the middle).

   A common operation for this will just be to *copy*, ie to have the
   same reading and writing subspace, and to do minimal processing in
   the middle. Empirically, this tends to coincide with the OV Circuit
   having (approximately) positive real eigenvalues. I mostly assert
   this as an empirical fact, but intuitively, operations that involve
   mapping eigenvectors to different directions (eg rotations) tend to
   have complex eigenvalues. And operations that preserve eigenvector
   direction but negate it tend to have negative real eigenvalues. And
   "what happens to the eigenvectors" is a decent proxy for what happens
   to an arbitrary vector.

   We can get a score for "how positive real the OV circuit eigenvalues
   are" with :math:`\frac{\sum \lambda_i}{\sum |\lambda_i|}`, where
   :math:`\lambda_i` are the eigenvalues of the OV circuit. This is a
   bit of a hack, but it seems to work well in practice.

.. container:: cell markdown

   Let's use FactoredMatrix to compute this for every head in the model!
   We use the helper ``model.OV`` to get the concatenated OV circuits
   for all heads across all layers in the model. This has the shape
   ``[n_layers, n_heads, d_model, d_model]``, where ``n_layers`` and
   ``n_heads`` are batch dimensions and the final two dimensions are
   factorised as ``[n_layers, n_heads, d_model, d_head]`` and
   ``[n_layers, n_heads, d_head, d_model]`` matrices.

   We can then get the eigenvalues for this, where there are separate
   eigenvalues for each element of the batch (a
   ``[n_layers, n_heads, d_head]`` tensor of complex numbers), and
   calculate the copying score.

.. container:: cell code

   .. code:: python

      OV_circuit_all_heads = model.OV
      print(OV_circuit_all_heads)

   .. container:: output stream stdout

      ::

         FactoredMatrix: Shape(torch.Size([12, 12, 768, 768])), Hidden Dim(64)

.. container:: cell code

   .. code:: python

      OV_circuit_all_heads_eigenvalues = OV_circuit_all_heads.eigenvalues 
      print(OV_circuit_all_heads_eigenvalues.shape)
      print(OV_circuit_all_heads_eigenvalues.dtype)

   .. container:: output stream stdout

      ::

         torch.Size([12, 12, 64])
         torch.complex64

.. container:: cell code

   .. code:: python

      OV_copying_score = OV_circuit_all_heads_eigenvalues.sum(dim=-1).real / OV_circuit_all_heads_eigenvalues.abs().sum(dim=-1)
      imshow(utils.to_numpy(OV_copying_score), xaxis="Head", yaxis="Layer", title="OV Copying Score for each head in GPT-2 Small", zmax=1.0, zmin=-1.0)

   .. container:: output display_data

      .. raw:: html

         <html>
         <head><meta charset="utf-8" /></head>
         <body>
             <div>            <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG"></script><script type="text/javascript">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: "STIX-Web"}});}</script>                <script type="text/javascript">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>
                 <script src="https://cdn.plot.ly/plotly-2.16.1.min.js"></script>                <div id="b1f0b6f8-feff-4327-9887-a72135ec4411" class="plotly-graph-div" style="height:525px; width:100%;"></div>            <script type="text/javascript">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById("b1f0b6f8-feff-4327-9887-a72135ec4411")) {                    Plotly.newPlot(                        "b1f0b6f8-feff-4327-9887-a72135ec4411",                        [{"coloraxis":"coloraxis","name":"0","z":[[0.777501,0.3527268,0.2596186,0.66702574,0.8384258,0.5584432,0.8444747,0.41379097,0.24488935,0.028157508,0.35840985,0.16288288],[-0.4541911,-0.6529327,-0.548457,-0.79903686,-0.7736425,-0.85225815,0.97743237,0.6626251,-0.73032224,-0.70070213,-0.6946625,-0.99967235],[-0.78371626,0.89677584,0.4750953,-0.6671974,0.7881463,-0.854775,-0.9054185,-0.57493836,-0.32175106,-0.028594071,-0.92476183,-0.9699269],[0.58640355,-0.7614349,0.59716946,0.7854394,-0.87888825,0.39087448,0.04473867,0.11028006,-0.8169987,0.22129549,-0.99395794,0.5774401],[0.52547896,0.30490145,-0.1072914,0.94331515,-0.931443,0.5273629,-0.4264709,-0.9984429,0.52967566,0.8604294,-0.88950515,0.955697],[0.66291875,0.42956945,0.9736858,0.65554816,0.12201875,0.7442768,0.5037956,0.95253587,-0.65071666,-0.931628,0.97915107,-0.99725866],[0.9613031,0.75017786,-0.38066593,0.64297867,0.95577693,-0.9428839,-0.994808,0.7852987,0.9657302,0.70730156,0.36872303,0.81280106],[0.9659483,0.9730121,0.3190061,-0.30290532,0.97909546,0.9357923,-0.5550317,-0.00546646,0.9867777,0.82495666,0.5664299,0.10005281],[-0.9464487,-0.2547201,0.6522326,0.14152558,0.98841417,0.98605835,0.6949271,0.9901811,0.97912025,-0.2359553,-0.98207104,0.65066874],[0.9895943,-0.29178143,0.9714024,0.9951602,0.18783762,-0.94609374,0.47801915,-0.24891911,0.94370985,0.11866241,0.99412435,-0.38088188],[0.9564488,0.5542725,0.4211806,0.66287893,0.8659593,0.99371177,0.9069077,0.39811078,-0.41342166,0.99719155,0.34596682,0.9938656],[0.58912665,0.9313741,0.9268401,0.9993564,0.62275416,0.8463948,0.6584346,0.8423125,0.29784963,0.8728679,0.99631435,0.98675257]],"type":"heatmap","xaxis":"x","yaxis":"y","hovertemplate":"Head: %{x}<br>Layer: %{y}<br>color: %{z}<extra></extra>"}],                        {"template":{"data":{"histogram2dcontour":[{"type":"histogram2dcontour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"choropleth":[{"type":"choropleth","colorbar":{"outlinewidth":0,"ticks":""}}],"histogram2d":[{"type":"histogram2d","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmap":[{"type":"heatmap","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmapgl":[{"type":"heatmapgl","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"contourcarpet":[{"type":"contourcarpet","colorbar":{"outlinewidth":0,"ticks":""}}],"contour":[{"type":"contour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"surface":[{"type":"surface","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"mesh3d":[{"type":"mesh3d","colorbar":{"outlinewidth":0,"ticks":""}}],"scatter":[{"fillpattern":{"fillmode":"overlay","size":10,"solidity":0.2},"type":"scatter"}],"parcoords":[{"type":"parcoords","line":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolargl":[{"type":"scatterpolargl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"scattergeo":[{"type":"scattergeo","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolar":[{"type":"scatterpolar","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"scattergl":[{"type":"scattergl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatter3d":[{"type":"scatter3d","line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattermapbox":[{"type":"scattermapbox","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterternary":[{"type":"scatterternary","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattercarpet":[{"type":"scattercarpet","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"pie":[{"automargin":true,"type":"pie"}]},"layout":{"autotypenumbers":"strict","colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"hovermode":"closest","hoverlabel":{"align":"left"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"bgcolor":"#E5ECF6","angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"ternary":{"bgcolor":"#E5ECF6","aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"sequential":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"sequentialminus":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]]},"xaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"yaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"geo":{"bgcolor":"white","landcolor":"#E5ECF6","subunitcolor":"white","showland":true,"showlakes":true,"lakecolor":"white"},"title":{"x":0.05},"mapbox":{"style":"light"}}},"xaxis":{"anchor":"y","domain":[0.0,1.0],"scaleanchor":"y","constrain":"domain","title":{"text":"Head"}},"yaxis":{"anchor":"x","domain":[0.0,1.0],"autorange":"reversed","constrain":"domain","title":{"text":"Layer"}},"coloraxis":{"colorscale":[[0.0,"rgb(103,0,31)"],[0.1,"rgb(178,24,43)"],[0.2,"rgb(214,96,77)"],[0.3,"rgb(244,165,130)"],[0.4,"rgb(253,219,199)"],[0.5,"rgb(247,247,247)"],[0.6,"rgb(209,229,240)"],[0.7,"rgb(146,197,222)"],[0.8,"rgb(67,147,195)"],[0.9,"rgb(33,102,172)"],[1.0,"rgb(5,48,97)"]],"cmid":0.0,"cmin":-1.0,"cmax":1.0},"title":{"text":"OV Copying Score for each head in GPT-2 Small"}},                        {"responsive": true}                    ).then(function(){
                                     
         var gd = document.getElementById('b1f0b6f8-feff-4327-9887-a72135ec4411');
         var x = new MutationObserver(function (mutations, observer) {{
                 var display = window.getComputedStyle(gd).display;
                 if (!display || display === 'none') {{
                     console.log([gd, 'removed!']);
                     Plotly.purge(gd);
                     observer.disconnect();
                 }}
         }});

         // Listen for the removal of the full notebook cells
         var notebookContainer = gd.closest('#notebook-container');
         if (notebookContainer) {{
             x.observe(notebookContainer, {childList: true});
         }}

         // Listen for the clearing of the current output cell
         var outputEl = gd.closest('.output');
         if (outputEl) {{
             x.observe(outputEl, {childList: true});
         }}

                                 })                };                            </script>        </div>
         </body>
         </html>

.. container:: cell markdown

   Head 11 in Layer 11 (L11H11) has a high copying score, and if we plot
   the eigenvalues they look approximately as expected.

.. container:: cell code

   .. code:: python

      scatter(x=OV_circuit_all_heads_eigenvalues[-1, -1, :].real, y=OV_circuit_all_heads_eigenvalues[-1, -1, :].imag, title="Eigenvalues of Head L11H11 of GPT-2 Small", xaxis="Real", yaxis="Imaginary")

   .. container:: output display_data

      .. raw:: html

         <html>
         <head><meta charset="utf-8" /></head>
         <body>
             <div>            <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG"></script><script type="text/javascript">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: "STIX-Web"}});}</script>                <script type="text/javascript">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>
                 <script src="https://cdn.plot.ly/plotly-2.16.1.min.js"></script>                <div id="bdd08916-2eb4-46b4-bcc5-896e7bc11cee" class="plotly-graph-div" style="height:525px; width:100%;"></div>            <script type="text/javascript">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById("bdd08916-2eb4-46b4-bcc5-896e7bc11cee")) {                    Plotly.newPlot(                        "bdd08916-2eb4-46b4-bcc5-896e7bc11cee",                        [{"hovertemplate":"Real=%{x}<br>Imaginary=%{y}<extra></extra>","legendgroup":"","marker":{"color":"#636efa","symbol":"circle"},"mode":"markers","name":"","orientation":"v","showlegend":false,"x":[-2.1397297,1.4152663,3.4444547,4.027667,8.882654,4.866768,4.866768,4.843705,4.843705,8.477541,8.216802,8.216802,5.0786076,7.8554554,7.8554554,5.365775,5.365775,5.5634394,5.5634394,5.4217362,7.7691364,7.7691364,7.042285,7.042285,5.6751432,5.6751432,7.678601,7.678601,6.5733356,6.5733356,7.6729503,7.172205,7.172205,7.4236083,7.4236083,7.4708047,6.089097,6.089097,6.306833,6.306833,6.5117445,6.5117445,5.955252,5.955252,5.8588047,5.8588047,7.1478877,7.1478877,7.185705,7.185705,6.6706057,6.6706057,6.735978,6.735978,6.1497555,6.1497555,6.2887864,6.2887864,6.3447866,6.625572,6.625572,6.899187,6.899187,6.856411],"xaxis":"x","y":[0.0,0.0,0.0,0.0,0.0,0.41852185,-0.41852185,0.09079792,-0.09079792,0.0,0.40868598,-0.40868598,0.0,0.7007218,-0.7007218,0.46421358,-0.46421358,0.5558223,-0.5558223,0.0,0.47056165,-0.47056165,1.0298702,-1.0298702,0.48252946,-0.48252946,0.33565187,-0.33565187,0.9988694,-0.9988694,0.0,0.7531841,-0.7531841,0.42575985,-0.42575985,0.0,0.6436292,-0.6436292,0.7701688,-0.7701688,0.7558016,-0.7558016,0.2591141,-0.2591141,0.013021447,-0.013021447,0.40166607,-0.40166607,0.28192392,-0.28192392,0.6146257,-0.6146257,0.5391219,-0.5391219,0.2823353,-0.2823353,0.35283586,-0.35283586,0.0,0.24868369,-0.24868369,0.15545632,-0.15545632,0.0],"yaxis":"y","type":"scatter"}],                        {"template":{"data":{"histogram2dcontour":[{"type":"histogram2dcontour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"choropleth":[{"type":"choropleth","colorbar":{"outlinewidth":0,"ticks":""}}],"histogram2d":[{"type":"histogram2d","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmap":[{"type":"heatmap","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmapgl":[{"type":"heatmapgl","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"contourcarpet":[{"type":"contourcarpet","colorbar":{"outlinewidth":0,"ticks":""}}],"contour":[{"type":"contour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"surface":[{"type":"surface","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"mesh3d":[{"type":"mesh3d","colorbar":{"outlinewidth":0,"ticks":""}}],"scatter":[{"fillpattern":{"fillmode":"overlay","size":10,"solidity":0.2},"type":"scatter"}],"parcoords":[{"type":"parcoords","line":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolargl":[{"type":"scatterpolargl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"scattergeo":[{"type":"scattergeo","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolar":[{"type":"scatterpolar","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"scattergl":[{"type":"scattergl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatter3d":[{"type":"scatter3d","line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattermapbox":[{"type":"scattermapbox","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterternary":[{"type":"scatterternary","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattercarpet":[{"type":"scattercarpet","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"pie":[{"automargin":true,"type":"pie"}]},"layout":{"autotypenumbers":"strict","colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"hovermode":"closest","hoverlabel":{"align":"left"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"bgcolor":"#E5ECF6","angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"ternary":{"bgcolor":"#E5ECF6","aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"sequential":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"sequentialminus":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]]},"xaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"yaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"geo":{"bgcolor":"white","landcolor":"#E5ECF6","subunitcolor":"white","showland":true,"showlakes":true,"lakecolor":"white"},"title":{"x":0.05},"mapbox":{"style":"light"}}},"xaxis":{"anchor":"y","domain":[0.0,1.0],"title":{"text":"Real"}},"yaxis":{"anchor":"x","domain":[0.0,1.0],"title":{"text":"Imaginary"}},"legend":{"tracegroupgap":0},"title":{"text":"Eigenvalues of Head L11H11 of GPT-2 Small"}},                        {"responsive": true}                    ).then(function(){
                                     
         var gd = document.getElementById('bdd08916-2eb4-46b4-bcc5-896e7bc11cee');
         var x = new MutationObserver(function (mutations, observer) {{
                 var display = window.getComputedStyle(gd).display;
                 if (!display || display === 'none') {{
                     console.log([gd, 'removed!']);
                     Plotly.purge(gd);
                     observer.disconnect();
                 }}
         }});

         // Listen for the removal of the full notebook cells
         var notebookContainer = gd.closest('#notebook-container');
         if (notebookContainer) {{
             x.observe(notebookContainer, {childList: true});
         }}

         // Listen for the clearing of the current output cell
         var outputEl = gd.closest('.output');
         if (outputEl) {{
             x.observe(outputEl, {childList: true});
         }}

                                 })                };                            </script>        </div>
         </body>
         </html>

.. container:: cell markdown

   We can even look at the full OV circuit, from the input tokens to
   output tokens: :math:`W_E W_V W_O W_U`. This is a
   ``[d_vocab, d_vocab]==[50257, 50257]`` matrix, so absolutely
   enormous, even for a single head. But with the FactoredMatrix class,
   we can compute the full eigenvalue copying score of every head in a
   few seconds.

.. container:: cell code

   .. code:: python

      full_OV_circuit = model.embed.W_E @ OV_circuit_all_heads @ model.unembed.W_U
      print(full_OV_circuit)

   .. container:: output stream stdout

      ::

         FactoredMatrix: Shape(torch.Size([12, 12, 50257, 50257])), Hidden Dim(64)

.. container:: cell code

   .. code:: python

      full_OV_circuit_eigenvalues = full_OV_circuit.eigenvalues
      print(full_OV_circuit_eigenvalues.shape)
      print(full_OV_circuit_eigenvalues.dtype)

   .. container:: output stream stdout

      ::

         torch.Size([12, 12, 64])
         torch.complex64

.. container:: cell code

   .. code:: python

      full_OV_copying_score = full_OV_circuit_eigenvalues.sum(dim=-1).real / full_OV_circuit_eigenvalues.abs().sum(dim=-1)
      imshow(utils.to_numpy(full_OV_copying_score), xaxis="Head", yaxis="Layer", title="OV Copying Score for each head in GPT-2 Small", zmax=1.0, zmin=-1.0)

   .. container:: output display_data

      .. raw:: html

         <html>
         <head><meta charset="utf-8" /></head>
         <body>
             <div>            <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG"></script><script type="text/javascript">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: "STIX-Web"}});}</script>                <script type="text/javascript">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>
                 <script src="https://cdn.plot.ly/plotly-2.16.1.min.js"></script>                <div id="ef099db3-850d-4f72-bcbc-a001170dfe46" class="plotly-graph-div" style="height:525px; width:100%;"></div>            <script type="text/javascript">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById("ef099db3-850d-4f72-bcbc-a001170dfe46")) {                    Plotly.newPlot(                        "ef099db3-850d-4f72-bcbc-a001170dfe46",                        [{"coloraxis":"coloraxis","name":"0","z":[[0.8356366,0.58535385,0.5105842,0.78433764,0.8644159,0.70265853,0.8969925,0.5868824,0.42486504,-0.16337514,0.46268544,0.27605426],[-0.052920163,-0.3177313,-0.48105782,-0.7838066,-0.63602084,-0.77586824,0.96818024,0.81191146,-0.75104624,-0.68784463,-0.64298844,-0.99858564],[-0.65983284,0.9152503,0.54614985,-0.4874397,0.7720561,-0.7541063,-0.847245,-0.6948985,-0.1557513,0.24442284,-0.91066235,-0.94391507],[0.6486897,-0.5592913,0.5935595,0.7843037,-0.8150344,0.6130046,0.16785872,0.3519588,-0.6837264,0.22237684,-0.99292195,0.65358186],[0.5740951,0.3640131,0.0960908,0.93596244,-0.9228775,0.61910796,-0.33572662,-0.998465,0.6448632,0.8468657,-0.7557656,0.95279706],[0.7326544,0.53241664,0.97326714,0.7239247,0.25538945,0.81584096,0.66557914,0.9287099,-0.5660435,-0.89087445,0.98342353,-0.99811804],[0.96986926,0.7439671,-0.35639316,0.6022988,0.9708116,-0.9278276,-0.9962316,0.8345208,0.9714328,0.8158544,0.5902574,0.8199346],[0.98202264,0.9859328,0.51524574,-0.56105167,0.96636665,0.9495159,-0.52048147,0.31047505,0.9859083,0.7797459,0.6738534,0.39197388],[-0.9062039,0.11750999,0.8077877,0.4169305,0.9829014,0.9902303,0.7847102,0.994563,0.9868026,-0.26804435,-0.99088675,0.74579275],[0.9906192,-0.18231122,0.97578365,0.99867505,0.25443316,-0.9544061,0.5869243,-0.23537976,0.9550504,0.25511968,0.992987,0.090525985],[0.9707273,0.6956093,0.6280023,0.79028654,0.934384,0.98957944,0.94362825,-0.10834958,-0.3431112,0.99867094,0.50867337,0.99495095],[0.8283133,0.9432441,0.9491768,0.9995353,0.57123184,0.8055236,0.67818654,0.8272576,0.8314806,0.8778659,0.99449587,0.99738663]],"type":"heatmap","xaxis":"x","yaxis":"y","hovertemplate":"Head: %{x}<br>Layer: %{y}<br>color: %{z}<extra></extra>"}],                        {"template":{"data":{"histogram2dcontour":[{"type":"histogram2dcontour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"choropleth":[{"type":"choropleth","colorbar":{"outlinewidth":0,"ticks":""}}],"histogram2d":[{"type":"histogram2d","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmap":[{"type":"heatmap","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmapgl":[{"type":"heatmapgl","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"contourcarpet":[{"type":"contourcarpet","colorbar":{"outlinewidth":0,"ticks":""}}],"contour":[{"type":"contour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"surface":[{"type":"surface","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"mesh3d":[{"type":"mesh3d","colorbar":{"outlinewidth":0,"ticks":""}}],"scatter":[{"fillpattern":{"fillmode":"overlay","size":10,"solidity":0.2},"type":"scatter"}],"parcoords":[{"type":"parcoords","line":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolargl":[{"type":"scatterpolargl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"scattergeo":[{"type":"scattergeo","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolar":[{"type":"scatterpolar","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"scattergl":[{"type":"scattergl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatter3d":[{"type":"scatter3d","line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattermapbox":[{"type":"scattermapbox","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterternary":[{"type":"scatterternary","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattercarpet":[{"type":"scattercarpet","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"pie":[{"automargin":true,"type":"pie"}]},"layout":{"autotypenumbers":"strict","colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"hovermode":"closest","hoverlabel":{"align":"left"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"bgcolor":"#E5ECF6","angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"ternary":{"bgcolor":"#E5ECF6","aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"sequential":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"sequentialminus":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]]},"xaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"yaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"geo":{"bgcolor":"white","landcolor":"#E5ECF6","subunitcolor":"white","showland":true,"showlakes":true,"lakecolor":"white"},"title":{"x":0.05},"mapbox":{"style":"light"}}},"xaxis":{"anchor":"y","domain":[0.0,1.0],"scaleanchor":"y","constrain":"domain","title":{"text":"Head"}},"yaxis":{"anchor":"x","domain":[0.0,1.0],"autorange":"reversed","constrain":"domain","title":{"text":"Layer"}},"coloraxis":{"colorscale":[[0.0,"rgb(103,0,31)"],[0.1,"rgb(178,24,43)"],[0.2,"rgb(214,96,77)"],[0.3,"rgb(244,165,130)"],[0.4,"rgb(253,219,199)"],[0.5,"rgb(247,247,247)"],[0.6,"rgb(209,229,240)"],[0.7,"rgb(146,197,222)"],[0.8,"rgb(67,147,195)"],[0.9,"rgb(33,102,172)"],[1.0,"rgb(5,48,97)"]],"cmid":0.0,"cmin":-1.0,"cmax":1.0},"title":{"text":"OV Copying Score for each head in GPT-2 Small"}},                        {"responsive": true}                    ).then(function(){
                                     
         var gd = document.getElementById('ef099db3-850d-4f72-bcbc-a001170dfe46');
         var x = new MutationObserver(function (mutations, observer) {{
                 var display = window.getComputedStyle(gd).display;
                 if (!display || display === 'none') {{
                     console.log([gd, 'removed!']);
                     Plotly.purge(gd);
                     observer.disconnect();
                 }}
         }});

         // Listen for the removal of the full notebook cells
         var notebookContainer = gd.closest('#notebook-container');
         if (notebookContainer) {{
             x.observe(notebookContainer, {childList: true});
         }}

         // Listen for the clearing of the current output cell
         var outputEl = gd.closest('.output');
         if (outputEl) {{
             x.observe(outputEl, {childList: true});
         }}

                                 })                };                            </script>        </div>
         </body>
         </html>

.. container:: cell markdown

   Interestingly, these are highly (but not perfectly!) correlated. I'm
   not sure what to read from this, or what's up with the weird outlier
   heads!

.. container:: cell code

   .. code:: python

      scatter(x=full_OV_copying_score.flatten(), y=OV_copying_score.flatten(), hover_name=[f"L{layer}H{head}" for layer in range(12) for head in range(12)], title="OV Copying Score for each head in GPT-2 Small", xaxis="Full OV Copying Score", yaxis="OV Copying Score")

   .. container:: output display_data

      .. raw:: html

         <html>
         <head><meta charset="utf-8" /></head>
         <body>
             <div>            <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG"></script><script type="text/javascript">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: "STIX-Web"}});}</script>                <script type="text/javascript">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>
                 <script src="https://cdn.plot.ly/plotly-2.16.1.min.js"></script>                <div id="df2e2140-8dfe-4259-a9ef-ccdd4f9b62ec" class="plotly-graph-div" style="height:525px; width:100%;"></div>            <script type="text/javascript">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById("df2e2140-8dfe-4259-a9ef-ccdd4f9b62ec")) {                    Plotly.newPlot(                        "df2e2140-8dfe-4259-a9ef-ccdd4f9b62ec",                        [{"hovertemplate":"<b>%{hovertext}</b><br><br>Full OV Copying Score=%{x}<br>OV Copying Score=%{y}<extra></extra>","hovertext":["L0H0","L0H1","L0H2","L0H3","L0H4","L0H5","L0H6","L0H7","L0H8","L0H9","L0H10","L0H11","L1H0","L1H1","L1H2","L1H3","L1H4","L1H5","L1H6","L1H7","L1H8","L1H9","L1H10","L1H11","L2H0","L2H1","L2H2","L2H3","L2H4","L2H5","L2H6","L2H7","L2H8","L2H9","L2H10","L2H11","L3H0","L3H1","L3H2","L3H3","L3H4","L3H5","L3H6","L3H7","L3H8","L3H9","L3H10","L3H11","L4H0","L4H1","L4H2","L4H3","L4H4","L4H5","L4H6","L4H7","L4H8","L4H9","L4H10","L4H11","L5H0","L5H1","L5H2","L5H3","L5H4","L5H5","L5H6","L5H7","L5H8","L5H9","L5H10","L5H11","L6H0","L6H1","L6H2","L6H3","L6H4","L6H5","L6H6","L6H7","L6H8","L6H9","L6H10","L6H11","L7H0","L7H1","L7H2","L7H3","L7H4","L7H5","L7H6","L7H7","L7H8","L7H9","L7H10","L7H11","L8H0","L8H1","L8H2","L8H3","L8H4","L8H5","L8H6","L8H7","L8H8","L8H9","L8H10","L8H11","L9H0","L9H1","L9H2","L9H3","L9H4","L9H5","L9H6","L9H7","L9H8","L9H9","L9H10","L9H11","L10H0","L10H1","L10H2","L10H3","L10H4","L10H5","L10H6","L10H7","L10H8","L10H9","L10H10","L10H11","L11H0","L11H1","L11H2","L11H3","L11H4","L11H5","L11H6","L11H7","L11H8","L11H9","L11H10","L11H11"],"legendgroup":"","marker":{"color":"#636efa","symbol":"circle"},"mode":"markers","name":"","orientation":"v","showlegend":false,"x":[0.8356366,0.58535385,0.5105842,0.78433764,0.8644159,0.70265853,0.8969925,0.5868824,0.42486504,-0.16337514,0.46268544,0.27605426,-0.052920163,-0.3177313,-0.48105782,-0.7838066,-0.63602084,-0.77586824,0.96818024,0.81191146,-0.75104624,-0.68784463,-0.64298844,-0.99858564,-0.65983284,0.9152503,0.54614985,-0.4874397,0.7720561,-0.7541063,-0.847245,-0.6948985,-0.1557513,0.24442284,-0.91066235,-0.94391507,0.6486897,-0.5592913,0.5935595,0.7843037,-0.8150344,0.6130046,0.16785872,0.3519588,-0.6837264,0.22237684,-0.99292195,0.65358186,0.5740951,0.3640131,0.0960908,0.93596244,-0.9228775,0.61910796,-0.33572662,-0.998465,0.6448632,0.8468657,-0.7557656,0.95279706,0.7326544,0.53241664,0.97326714,0.7239247,0.25538945,0.81584096,0.66557914,0.9287099,-0.5660435,-0.89087445,0.98342353,-0.99811804,0.96986926,0.7439671,-0.35639316,0.6022988,0.9708116,-0.9278276,-0.9962316,0.8345208,0.9714328,0.8158544,0.5902574,0.8199346,0.98202264,0.9859328,0.51524574,-0.56105167,0.96636665,0.9495159,-0.52048147,0.31047505,0.9859083,0.7797459,0.6738534,0.39197388,-0.9062039,0.11750999,0.8077877,0.4169305,0.9829014,0.9902303,0.7847102,0.994563,0.9868026,-0.26804435,-0.99088675,0.74579275,0.9906192,-0.18231122,0.97578365,0.99867505,0.25443316,-0.9544061,0.5869243,-0.23537976,0.9550504,0.25511968,0.992987,0.090525985,0.9707273,0.6956093,0.6280023,0.79028654,0.934384,0.98957944,0.94362825,-0.10834958,-0.3431112,0.99867094,0.50867337,0.99495095,0.8283133,0.9432441,0.9491768,0.9995353,0.57123184,0.8055236,0.67818654,0.8272576,0.8314806,0.8778659,0.99449587,0.99738663],"xaxis":"x","y":[0.777501,0.3527268,0.2596186,0.66702574,0.8384258,0.5584432,0.8444747,0.41379097,0.24488935,0.028157508,0.35840985,0.16288288,-0.4541911,-0.6529327,-0.548457,-0.79903686,-0.7736425,-0.85225815,0.97743237,0.6626251,-0.73032224,-0.70070213,-0.6946625,-0.99967235,-0.78371626,0.89677584,0.4750953,-0.6671974,0.7881463,-0.854775,-0.9054185,-0.57493836,-0.32175106,-0.028594071,-0.92476183,-0.9699269,0.58640355,-0.7614349,0.59716946,0.7854394,-0.87888825,0.39087448,0.04473867,0.11028006,-0.8169987,0.22129549,-0.99395794,0.5774401,0.52547896,0.30490145,-0.1072914,0.94331515,-0.931443,0.5273629,-0.4264709,-0.9984429,0.52967566,0.8604294,-0.88950515,0.955697,0.66291875,0.42956945,0.9736858,0.65554816,0.12201875,0.7442768,0.5037956,0.95253587,-0.65071666,-0.931628,0.97915107,-0.99725866,0.9613031,0.75017786,-0.38066593,0.64297867,0.95577693,-0.9428839,-0.994808,0.7852987,0.9657302,0.70730156,0.36872303,0.81280106,0.9659483,0.9730121,0.3190061,-0.30290532,0.97909546,0.9357923,-0.5550317,-0.00546646,0.9867777,0.82495666,0.5664299,0.10005281,-0.9464487,-0.2547201,0.6522326,0.14152558,0.98841417,0.98605835,0.6949271,0.9901811,0.97912025,-0.2359553,-0.98207104,0.65066874,0.9895943,-0.29178143,0.9714024,0.9951602,0.18783762,-0.94609374,0.47801915,-0.24891911,0.94370985,0.11866241,0.99412435,-0.38088188,0.9564488,0.5542725,0.4211806,0.66287893,0.8659593,0.99371177,0.9069077,0.39811078,-0.41342166,0.99719155,0.34596682,0.9938656,0.58912665,0.9313741,0.9268401,0.9993564,0.62275416,0.8463948,0.6584346,0.8423125,0.29784963,0.8728679,0.99631435,0.98675257],"yaxis":"y","type":"scatter"}],                        {"template":{"data":{"histogram2dcontour":[{"type":"histogram2dcontour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"choropleth":[{"type":"choropleth","colorbar":{"outlinewidth":0,"ticks":""}}],"histogram2d":[{"type":"histogram2d","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmap":[{"type":"heatmap","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmapgl":[{"type":"heatmapgl","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"contourcarpet":[{"type":"contourcarpet","colorbar":{"outlinewidth":0,"ticks":""}}],"contour":[{"type":"contour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"surface":[{"type":"surface","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"mesh3d":[{"type":"mesh3d","colorbar":{"outlinewidth":0,"ticks":""}}],"scatter":[{"fillpattern":{"fillmode":"overlay","size":10,"solidity":0.2},"type":"scatter"}],"parcoords":[{"type":"parcoords","line":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolargl":[{"type":"scatterpolargl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"scattergeo":[{"type":"scattergeo","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolar":[{"type":"scatterpolar","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"scattergl":[{"type":"scattergl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatter3d":[{"type":"scatter3d","line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattermapbox":[{"type":"scattermapbox","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterternary":[{"type":"scatterternary","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattercarpet":[{"type":"scattercarpet","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"pie":[{"automargin":true,"type":"pie"}]},"layout":{"autotypenumbers":"strict","colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"hovermode":"closest","hoverlabel":{"align":"left"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"bgcolor":"#E5ECF6","angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"ternary":{"bgcolor":"#E5ECF6","aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"sequential":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"sequentialminus":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]]},"xaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"yaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"geo":{"bgcolor":"white","landcolor":"#E5ECF6","subunitcolor":"white","showland":true,"showlakes":true,"lakecolor":"white"},"title":{"x":0.05},"mapbox":{"style":"light"}}},"xaxis":{"anchor":"y","domain":[0.0,1.0],"title":{"text":"Full OV Copying Score"}},"yaxis":{"anchor":"x","domain":[0.0,1.0],"title":{"text":"OV Copying Score"}},"legend":{"tracegroupgap":0},"title":{"text":"OV Copying Score for each head in GPT-2 Small"}},                        {"responsive": true}                    ).then(function(){
                                     
         var gd = document.getElementById('df2e2140-8dfe-4259-a9ef-ccdd4f9b62ec');
         var x = new MutationObserver(function (mutations, observer) {{
                 var display = window.getComputedStyle(gd).display;
                 if (!display || display === 'none') {{
                     console.log([gd, 'removed!']);
                     Plotly.purge(gd);
                     observer.disconnect();
                 }}
         }});

         // Listen for the removal of the full notebook cells
         var notebookContainer = gd.closest('#notebook-container');
         if (notebookContainer) {{
             x.observe(notebookContainer, {childList: true});
         }}

         // Listen for the clearing of the current output cell
         var outputEl = gd.closest('.output');
         if (outputEl) {{
             x.observe(outputEl, {childList: true});
         }}

                                 })                };                            </script>        </div>
         </body>
         </html>

.. container:: cell code

   .. code:: python

      print(f"Token 256 - the most common pair of ASCII characters: |{model.to_string(256)}|")
      # Squeeze means to remove dimensions of length 1. 
      # Here, that removes the dummy batch dimension so it's a rank 1 tensor and returns a string
      # Rank 2 tensors map to a list of strings
      print(f"De-Tokenizing the example tokens: {model.to_string(example_text_tokens.squeeze())}")

   .. container:: output stream stdout

      ::

         Token 256 - the most common pair of ASCII characters: | t|
         De-Tokenizing the example tokens: <|endoftext|>The first thing you need to figure out is *how* things are tokenized. `model.to_str_tokens` splits a string into the tokens *as a list of substrings*, and so lets you explore what the text looks like. To demonstrate this, let's use it on this paragraph.

.. container:: cell markdown

   .. rubric:: Generating Text
      :name: generating-text

.. container:: cell markdown

   TransformerLens also has basic text generation functionality, which
   can be useful for generally exploring what the model is capable of
   (thanks to Ansh Radhakrishnan for adding this!). This is pretty rough
   functionality, and where possible I recommend using more established
   libraries like HuggingFace for this.

.. container:: cell code

   .. code:: python

      model.generate("(CNN) President Barack Obama caught in embarrassing new scandal\n", max_new_tokens=50, temperature=0.7, prepend_bos=True)

   .. container:: output display_data

      .. code:: json

         {"version_major":2,"version_minor":0,"model_id":"9c541056beba477db0a272faec9231b5"}

   .. container:: output execute_result

      ::

         '(CNN) President Barack Obama caught in embarrassing new scandal\n\nCNN reported Wednesday that the former president was charged with lying to the FBI about his relationship with a Russian employee.\n\nThe FBI said in a statement that it had "conducted the investigation" into the matter.\n\nObama has pleaded not'

.. container:: cell markdown

   .. rubric:: Hook Points
      :name: hook-points

   The key part of TransformerLens that lets us access and edit
   intermediate activations are the HookPoints around every model
   activation. Importantly, this technique will work for *any* model
   architecture, not just transformers, so long as you're able to edit
   the model code to add in HookPoints! This is essentially a
   lightweight library bundled with TransformerLens that should let you
   take an arbitrary model and make it easier to study.

.. container:: cell markdown

   This is implemented by having a HookPoint layer. Each transformer
   component has a HookPoint for every activation, which wraps around
   that activation. The HookPoint acts as an identity function, but has
   a variety of helper functions that allows us to put PyTorch hooks in
   to edit and access the relevant activation.

   There is also a ``HookedRootModule`` class - this is a utility class
   that the root module should inherit from (root module = the model we
   run) - it has several utility functions for using hooks well, notably
   ``reset_hooks``, ``run_with_cache`` and ``run_with_hooks``.

   The default interface is the ``run_with_hooks`` function on the root
   module, which lets us run a forwards pass on the model, and pass on a
   list of hooks paired with layer names to run on that pass.

   The syntax for a hook is ``function(activation, hook)`` where
   ``activation`` is the activation the hook is wrapped around, and
   ``hook`` is the ``HookPoint`` class the function is attached to. If
   the function returns a new activation or edits the activation
   in-place, that replaces the old one, if it returns None then the
   activation remains as is.

.. container:: cell markdown

   .. rubric:: Toy Example
      :name: toy-example

.. container:: cell markdown

   Here's a simple example of defining a small network with HookPoints:

   We define a basic network with two layers that each take a scalar
   input :math:`x`, square it, and add a constant: :math:`x_0=x`,
   :math:`x_1=x_0^2+3`, :math:`x_2=x_1^2-4`.

   We wrap the input, each layer's output, and the intermediate value of
   each layer (the square) in a hook point.

.. container:: cell code

   .. code:: python


      from transformer_lens.hook_points import HookedRootModule, HookPoint


      class SquareThenAdd(nn.Module):
          def __init__(self, offset):
              super().__init__()
              self.offset = nn.Parameter(torch.tensor(offset))
              self.hook_square = HookPoint()

          def forward(self, x):
              # The hook_square doesn't change the value, but lets us access it
              square = self.hook_square(x * x)
              return self.offset + square


      class TwoLayerModel(HookedRootModule):
          def __init__(self):
              super().__init__()
              self.layer1 = SquareThenAdd(3.0)
              self.layer2 = SquareThenAdd(-4.0)
              self.hook_in = HookPoint()
              self.hook_mid = HookPoint()
              self.hook_out = HookPoint()

              # We need to call the setup function of HookedRootModule to build an
              # internal dictionary of modules and hooks, and to give each hook a name
              super().setup()

          def forward(self, x):
              # We wrap the input and each layer's output in a hook - they leave the
              # value unchanged (unless there's a hook added to explicitly change it),
              # but allow us to access it.
              x_in = self.hook_in(x)
              x_mid = self.hook_mid(self.layer1(x_in))
              x_out = self.hook_out(self.layer2(x_mid))
              return x_out


      model = TwoLayerModel()

.. container:: cell markdown

   We can add a cache, to save the activation at each hook point

   (There's a custom ``run_with_cache`` function on the root module as a
   convenience, which is a wrapper around model.forward that return
   model_out, cache_object - we could also manually add hooks with
   ``run_with_hooks`` that store activations in a global caching
   dictionary. This is often useful if we only want to store, eg,
   subsets or functions of some activations.)

.. container:: cell code

   .. code:: python


      out, cache = model.run_with_cache(torch.tensor(5.0))
      print("Model output:", out.item())
      for key in cache:
          print(f"Value cached at hook {key}", cache[key].item())

   .. container:: output stream stdout

      ::

         Model output: 780.0
         Value cached at hook hook_in 5.0
         Value cached at hook layer1.hook_square 25.0
         Value cached at hook hook_mid 28.0
         Value cached at hook layer2.hook_square 784.0
         Value cached at hook hook_out 780.0

.. container:: cell markdown

   We can also use hooks to intervene on activations - eg, we can set
   the intermediate value in layer 2 to zero to change the output to -5

.. container:: cell code

   .. code:: python


      def set_to_zero_hook(tensor, hook):
          print(hook.name)
          return torch.tensor(0.0)


      print(
          "Output after intervening on layer2.hook_scaled",
          model.run_with_hooks(
              torch.tensor(5.0), fwd_hooks=[("layer2.hook_square", set_to_zero_hook)]
          ).item(),
      )

   .. container:: output stream stdout

      ::

         layer2.hook_square
         Output after intervening on layer2.hook_scaled -4.0

.. container:: cell markdown

   .. rubric:: Loading Pre-Trained Checkpoints
      :name: loading-pre-trained-checkpoints

   There are a lot of interesting questions combining mechanistic
   interpretability and training dynamics - analysing model capabilities
   and the underlying circuits that make them possible, and how these
   change as we train the model.

   TransformerLens supports these by having several model families with
   checkpoints throughout training.
   ``HookedTransformer.from_pretrained`` can load a checkpoint of a
   model with the ``checkpoint_index`` (the label 0 to
   ``num_checkpoints-1``) or ``checkpoint_value`` (the step or token
   number, depending on how the checkpoints were labelled).

.. container:: cell markdown

   Available models:

   -  All of my interpretability-friendly models have checkpoints
      available, including:

      -  The toy models - ``attn-only``, ``solu``, ``gelu`` 1L to 4L

         -  These have ~200 checkpoints, taken on a piecewise linear
            schedule (more checkpoints near the start of training), up
            to 22B tokens. Labelled by number of tokens seen.

      -  The SoLU models trained on 80% Web Text and 20% Python Code
         (``solu-6l`` to ``solu-12l``)

         -  Same checkpoint schedule as the toy models, this time up to
            30B tokens

      -  The SoLU models trained on the pile (``solu-1l-pile`` to
         ``solu-12l-pile``)

         -  These have ~100 checkpoints, taken on a linear schedule, up
            to 15B tokens. Labelled by number of steps.
         -  The 12L training crashed around 11B tokens, so is truncated.

   -  The Stanford Centre for Research of Foundation Models trained 5
      GPT-2 Small sized and 5 GPT-2 Medium sized models
      (``stanford-gpt2-small-a`` to ``e`` and ``stanford-gpt2-medium-a``
      to ``e``)

      -  600 checkpoints, taken on a piecewise linear schedule, labelled
         by the number of steps.

.. container:: cell markdown

   The checkpoint structure and labels is somewhat messy and ad-hoc, so
   I mostly recommend using the ``checkpoint_index`` syntax (where you
   can just count from 0 to the number of checkpoints) rather than
   ``checkpoint_value`` syntax (where you need to know the checkpoint
   schedule, and whether it was labelled with the number of tokens or
   steps). The helper function ``get_checkpoint_labels`` tells you the
   checkpoint schedule for a given model - ie what point was each
   checkpoint taken at, and what type of label was used.

   Here are graphs of the schedules for several checkpointed models:
   (note that the first 3 use a log scale, latter 2 use a linear scale)

.. container:: cell code

   .. code:: python

      from transformer_lens.loading_from_pretrained import get_checkpoint_labels
      for model_name in ["attn-only-2l", "solu-12l", "stanford-gpt2-small-a"]:
          checkpoint_labels, checkpoint_label_type = get_checkpoint_labels(model_name)
          line(checkpoint_labels, xaxis="Checkpoint Index", yaxis=f"Checkpoint Value ({checkpoint_label_type})", title=f"Checkpoint Values for {model_name} (Log scale)", log_y=True, markers=True)
      for model_name in ["solu-1l-pile", "solu-6l-pile"]:
          checkpoint_labels, checkpoint_label_type = get_checkpoint_labels(model_name)
          line(checkpoint_labels, xaxis="Checkpoint Index", yaxis=f"Checkpoint Value ({checkpoint_label_type})", title=f"Checkpoint Values for {model_name} (Linear scale)", log_y=False, markers=True)

   .. container:: output display_data

      .. raw:: html

         <html>
         <head><meta charset="utf-8" /></head>
         <body>
             <div>            <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG"></script><script type="text/javascript">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: "STIX-Web"}});}</script>                <script type="text/javascript">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>
                 <script src="https://cdn.plot.ly/plotly-2.16.1.min.js"></script>                <div id="c7196983-521b-4ec3-a93d-e9a83d4888ad" class="plotly-graph-div" style="height:525px; width:100%;"></div>            <script type="text/javascript">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById("c7196983-521b-4ec3-a93d-e9a83d4888ad")) {                    Plotly.newPlot(                        "c7196983-521b-4ec3-a93d-e9a83d4888ad",                        [{"hovertemplate":"variable=0<br>index=%{x}<br>value=%{y}<extra></extra>","legendgroup":"0","line":{"color":"#636efa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"markers+lines","name":"0","orientation":"v","showlegend":true,"x":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162],"xaxis":"x","y":[262144,2621440,4718592,7077888,9175040,11272192,13631488,15728640,18087936,20185088,22282240,33292288,44302336,55312384,66322432,77332480,88342528,99352576,110362624,121372672,132382720,143392768,154402816,165412864,176422912,187432960,198443008,209453056,220463104,264503296,308281344,352321536,396361728,440401920,484442112,528482304,572522496,616300544,660340736,704380928,748421120,792461312,836501504,880279552,924319744,968359936,1012400128,1056440320,1100480512,1144520704,1188298752,1232338944,1276379136,1320419328,1364459520,1408499712,1452277760,1496317952,1540358144,1584398336,1628438528,1672478720,1716518912,1760296960,1804337152,1848377344,1892417536,1936457728,1980497920,2024275968,2068316160,2112356352,2156396544,2200436736,2420375552,2640314368,2860515328,3080454144,3300392960,3520331776,3740270592,3960471552,4180410368,4400349184,4620288000,4840488960,5060427776,5280366592,5500305408,5720506368,5940445184,6160384000,6380322816,6600523776,6820462592,7040401408,7260340224,7480279040,7700480000,7920418816,8140357632,8360296448,8580497408,8800436224,9020375040,9240313856,9460514816,9680453632,9900392448,10120331264,10340270080,10560471040,10780409856,11000348672,11220287488,11440488448,11660427264,11880366080,12100304896,12320505856,12540444672,12760383488,12980322304,13200523264,13420462080,13640400896,13860339712,14080278528,14300479488,14520418304,14740357120,14960295936,15180496896,15400435712,15620374528,15840313344,16060514304,16280453120,16500391936,16720330752,16940269568,17160470528,17380409344,17600348160,17820286976,18040487936,18260426752,18480365568,18700304384,18920505344,19140444160,19360382976,19580321792,19800522752,20020461568,20240400384,20460339200,20680278016,20900478976,21120417792,21340356608,21560295424,21780496384],"yaxis":"y","type":"scatter"}],                        {"template":{"data":{"histogram2dcontour":[{"type":"histogram2dcontour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"choropleth":[{"type":"choropleth","colorbar":{"outlinewidth":0,"ticks":""}}],"histogram2d":[{"type":"histogram2d","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmap":[{"type":"heatmap","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmapgl":[{"type":"heatmapgl","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"contourcarpet":[{"type":"contourcarpet","colorbar":{"outlinewidth":0,"ticks":""}}],"contour":[{"type":"contour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"surface":[{"type":"surface","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"mesh3d":[{"type":"mesh3d","colorbar":{"outlinewidth":0,"ticks":""}}],"scatter":[{"fillpattern":{"fillmode":"overlay","size":10,"solidity":0.2},"type":"scatter"}],"parcoords":[{"type":"parcoords","line":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolargl":[{"type":"scatterpolargl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"scattergeo":[{"type":"scattergeo","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolar":[{"type":"scatterpolar","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"scattergl":[{"type":"scattergl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatter3d":[{"type":"scatter3d","line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattermapbox":[{"type":"scattermapbox","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterternary":[{"type":"scatterternary","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattercarpet":[{"type":"scattercarpet","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"pie":[{"automargin":true,"type":"pie"}]},"layout":{"autotypenumbers":"strict","colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"hovermode":"closest","hoverlabel":{"align":"left"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"bgcolor":"#E5ECF6","angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"ternary":{"bgcolor":"#E5ECF6","aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"sequential":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"sequentialminus":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]]},"xaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"yaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"geo":{"bgcolor":"white","landcolor":"#E5ECF6","subunitcolor":"white","showland":true,"showlakes":true,"lakecolor":"white"},"title":{"x":0.05},"mapbox":{"style":"light"}}},"xaxis":{"anchor":"y","domain":[0.0,1.0],"title":{"text":"index"}},"yaxis":{"anchor":"x","domain":[0.0,1.0],"title":{"text":"value"},"type":"log"},"legend":{"title":{"text":"variable"},"tracegroupgap":0},"title":{"text":"Checkpoint Values for attn-only-2l (Log scale)"}},                        {"responsive": true}                    ).then(function(){
                                     
         var gd = document.getElementById('c7196983-521b-4ec3-a93d-e9a83d4888ad');
         var x = new MutationObserver(function (mutations, observer) {{
                 var display = window.getComputedStyle(gd).display;
                 if (!display || display === 'none') {{
                     console.log([gd, 'removed!']);
                     Plotly.purge(gd);
                     observer.disconnect();
                 }}
         }});

         // Listen for the removal of the full notebook cells
         var notebookContainer = gd.closest('#notebook-container');
         if (notebookContainer) {{
             x.observe(notebookContainer, {childList: true});
         }}

         // Listen for the clearing of the current output cell
         var outputEl = gd.closest('.output');
         if (outputEl) {{
             x.observe(outputEl, {childList: true});
         }}

                                 })                };                            </script>        </div>
         </body>
         </html>

   .. container:: output display_data

      .. raw:: html

         <html>
         <head><meta charset="utf-8" /></head>
         <body>
             <div>            <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG"></script><script type="text/javascript">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: "STIX-Web"}});}</script>                <script type="text/javascript">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>
                 <script src="https://cdn.plot.ly/plotly-2.16.1.min.js"></script>                <div id="6ada1b24-ab2d-4a0e-b3a0-b25c42a2762e" class="plotly-graph-div" style="height:525px; width:100%;"></div>            <script type="text/javascript">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById("6ada1b24-ab2d-4a0e-b3a0-b25c42a2762e")) {                    Plotly.newPlot(                        "6ada1b24-ab2d-4a0e-b3a0-b25c42a2762e",                        [{"hovertemplate":"variable=0<br>index=%{x}<br>value=%{y}<extra></extra>","legendgroup":"0","line":{"color":"#636efa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"markers+lines","name":"0","orientation":"v","showlegend":true,"x":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162],"xaxis":"x","y":[196608,3342336,6291456,9240576,12386304,15335424,18284544,21233664,24379392,27328512,30277632,45219840,60358656,75300864,90243072,105381888,120324096,135266304,150208512,165347328,180289536,195231744,210370560,225312768,240254976,255197184,270336000,285278208,300220416,360382464,420347904,480313344,540278784,600244224,660209664,720371712,780337152,840302592,900268032,960233472,1020198912,1080360960,1140326400,1200291840,1260257280,1320222720,1380384768,1440350208,1500315648,1560281088,1620246528,1680211968,1740374016,1800339456,1860304896,1920270336,1980235776,2040201216,2100363264,2160328704,2220294144,2280259584,2340225024,2400387072,2460352512,2520317952,2580283392,2640248832,2700214272,2760376320,2820341760,2880307200,2940272640,3000238080,3300261888,3600285696,3900309504,4200333312,4500357120,4800380928,5100208128,5400231936,5700255744,6000279552,6300303360,6600327168,6900350976,7200374784,7500201984,7800225792,8100249600,8400273408,8700297216,9000321024,9300344832,9600368640,9900392448,10200219648,10500243456,10800267264,11100291072,11400314880,11700338688,12000362496,12300386304,12600213504,12900237312,13200261120,13500284928,13800308736,14100332544,14400356352,14700380160,15000207360,15300231168,15600254976,15900278784,16200302592,16500326400,16800350208,17100374016,17400201216,17700225024,18000248832,18300272640,18600296448,18900320256,19200344064,19500367872,19800391680,20100218880,20400242688,20700266496,21000290304,21300314112,21600337920,21900361728,22200385536,22500212736,22800236544,23100260352,23400284160,23700307968,24000331776,24300355584,24600379392,24900206592,25200230400,25500254208,25800278016,26100301824,26400325632,26700349440,27000373248,27300200448,27600224256,27900248064,28200271872,28500295680,28800319488,29100343296,29400367104,29700390912],"yaxis":"y","type":"scatter"}],                        {"template":{"data":{"histogram2dcontour":[{"type":"histogram2dcontour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"choropleth":[{"type":"choropleth","colorbar":{"outlinewidth":0,"ticks":""}}],"histogram2d":[{"type":"histogram2d","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmap":[{"type":"heatmap","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmapgl":[{"type":"heatmapgl","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"contourcarpet":[{"type":"contourcarpet","colorbar":{"outlinewidth":0,"ticks":""}}],"contour":[{"type":"contour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"surface":[{"type":"surface","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"mesh3d":[{"type":"mesh3d","colorbar":{"outlinewidth":0,"ticks":""}}],"scatter":[{"fillpattern":{"fillmode":"overlay","size":10,"solidity":0.2},"type":"scatter"}],"parcoords":[{"type":"parcoords","line":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolargl":[{"type":"scatterpolargl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"scattergeo":[{"type":"scattergeo","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolar":[{"type":"scatterpolar","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"scattergl":[{"type":"scattergl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatter3d":[{"type":"scatter3d","line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattermapbox":[{"type":"scattermapbox","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterternary":[{"type":"scatterternary","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattercarpet":[{"type":"scattercarpet","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"pie":[{"automargin":true,"type":"pie"}]},"layout":{"autotypenumbers":"strict","colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"hovermode":"closest","hoverlabel":{"align":"left"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"bgcolor":"#E5ECF6","angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"ternary":{"bgcolor":"#E5ECF6","aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"sequential":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"sequentialminus":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]]},"xaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"yaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"geo":{"bgcolor":"white","landcolor":"#E5ECF6","subunitcolor":"white","showland":true,"showlakes":true,"lakecolor":"white"},"title":{"x":0.05},"mapbox":{"style":"light"}}},"xaxis":{"anchor":"y","domain":[0.0,1.0],"title":{"text":"index"}},"yaxis":{"anchor":"x","domain":[0.0,1.0],"title":{"text":"value"},"type":"log"},"legend":{"title":{"text":"variable"},"tracegroupgap":0},"title":{"text":"Checkpoint Values for solu-12l (Log scale)"}},                        {"responsive": true}                    ).then(function(){
                                     
         var gd = document.getElementById('6ada1b24-ab2d-4a0e-b3a0-b25c42a2762e');
         var x = new MutationObserver(function (mutations, observer) {{
                 var display = window.getComputedStyle(gd).display;
                 if (!display || display === 'none') {{
                     console.log([gd, 'removed!']);
                     Plotly.purge(gd);
                     observer.disconnect();
                 }}
         }});

         // Listen for the removal of the full notebook cells
         var notebookContainer = gd.closest('#notebook-container');
         if (notebookContainer) {{
             x.observe(notebookContainer, {childList: true});
         }}

         // Listen for the clearing of the current output cell
         var outputEl = gd.closest('.output');
         if (outputEl) {{
             x.observe(outputEl, {childList: true});
         }}

                                 })                };                            </script>        </div>
         </body>
         </html>

   .. container:: output display_data

      .. raw:: html

         <html>
         <head><meta charset="utf-8" /></head>
         <body>
             <div>            <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG"></script><script type="text/javascript">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: "STIX-Web"}});}</script>                <script type="text/javascript">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>
                 <script src="https://cdn.plot.ly/plotly-2.16.1.min.js"></script>                <div id="87c7d210-cc47-4c30-be57-2c8152b6d491" class="plotly-graph-div" style="height:525px; width:100%;"></div>            <script type="text/javascript">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById("87c7d210-cc47-4c30-be57-2c8152b6d491")) {                    Plotly.newPlot(                        "87c7d210-cc47-4c30-be57-2c8152b6d491",                        [{"hovertemplate":"variable=0<br>index=%{x}<br>value=%{y}<extra></extra>","legendgroup":"0","line":{"color":"#636efa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"markers+lines","name":"0","orientation":"v","showlegend":true,"x":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541,542,543,544,545,546,547,548,549,550,551,552,553,554,555,556,557,558,559,560,561,562,563,564,565,566,567,568,569,570,571,572,573,574,575,576,577,578,579,580,581,582,583,584,585,586,587,588,589,590,591,592,593,594,595,596,597,598,599,600,601,602,603,604,605,606,607,608],"xaxis":"x","y":[0,10,20,30,40,50,60,70,80,90,100,150,200,250,300,350,400,450,500,550,600,650,700,750,800,850,900,950,1000,1050,1100,1150,1200,1250,1300,1350,1400,1450,1500,1550,1600,1650,1700,1750,1800,1850,1900,1950,2000,2100,2200,2300,2400,2500,2600,2700,2800,2900,3000,3100,3200,3300,3400,3500,3600,3700,3800,3900,4000,4100,4200,4300,4400,4500,4600,4700,4800,4900,5000,5100,5200,5300,5400,5500,5600,5700,5800,5900,6000,6100,6200,6300,6400,6500,6600,6700,6800,6900,7000,7100,7200,7300,7400,7500,7600,7700,7800,7900,8000,8100,8200,8300,8400,8500,8600,8700,8800,8900,9000,9100,9200,9300,9400,9500,9600,9700,9800,9900,10000,10100,10200,10300,10400,10500,10600,10700,10800,10900,11000,11100,11200,11300,11400,11500,11600,11700,11800,11900,12000,12100,12200,12300,12400,12500,12600,12700,12800,12900,13000,13100,13200,13300,13400,13500,13600,13700,13800,13900,14000,14100,14200,14300,14400,14500,14600,14700,14800,14900,15000,15100,15200,15300,15400,15500,15600,15700,15800,15900,16000,16100,16200,16300,16400,16500,16600,16700,16800,16900,17000,17100,17200,17300,17400,17500,17600,17700,17800,17900,18000,18100,18200,18300,18400,18500,18600,18700,18800,18900,19000,19100,19200,19300,19400,19500,19600,19700,19800,19900,20000,21000,22000,23000,24000,25000,26000,27000,28000,29000,30000,31000,32000,33000,34000,35000,36000,37000,38000,39000,40000,41000,42000,43000,44000,45000,46000,47000,48000,49000,50000,51000,52000,53000,54000,55000,56000,57000,58000,59000,60000,61000,62000,63000,64000,65000,66000,67000,68000,69000,70000,71000,72000,73000,74000,75000,76000,77000,78000,79000,80000,81000,82000,83000,84000,85000,86000,87000,88000,89000,90000,91000,92000,93000,94000,95000,96000,97000,98000,99000,100000,101000,102000,103000,104000,105000,106000,107000,108000,109000,110000,111000,112000,113000,114000,115000,116000,117000,118000,119000,120000,121000,122000,123000,124000,125000,126000,127000,128000,129000,130000,131000,132000,133000,134000,135000,136000,137000,138000,139000,140000,141000,142000,143000,144000,145000,146000,147000,148000,149000,150000,151000,152000,153000,154000,155000,156000,157000,158000,159000,160000,161000,162000,163000,164000,165000,166000,167000,168000,169000,170000,171000,172000,173000,174000,175000,176000,177000,178000,179000,180000,181000,182000,183000,184000,185000,186000,187000,188000,189000,190000,191000,192000,193000,194000,195000,196000,197000,198000,199000,200000,201000,202000,203000,204000,205000,206000,207000,208000,209000,210000,211000,212000,213000,214000,215000,216000,217000,218000,219000,220000,221000,222000,223000,224000,225000,226000,227000,228000,229000,230000,231000,232000,233000,234000,235000,236000,237000,238000,239000,240000,241000,242000,243000,244000,245000,246000,247000,248000,249000,250000,251000,252000,253000,254000,255000,256000,257000,258000,259000,260000,261000,262000,263000,264000,265000,266000,267000,268000,269000,270000,271000,272000,273000,274000,275000,276000,277000,278000,279000,280000,281000,282000,283000,284000,285000,286000,287000,288000,289000,290000,291000,292000,293000,294000,295000,296000,297000,298000,299000,300000,301000,302000,303000,304000,305000,306000,307000,308000,309000,310000,311000,312000,313000,314000,315000,316000,317000,318000,319000,320000,321000,322000,323000,324000,325000,326000,327000,328000,329000,330000,331000,332000,333000,334000,335000,336000,337000,338000,339000,340000,341000,342000,343000,344000,345000,346000,347000,348000,349000,350000,351000,352000,353000,354000,355000,356000,357000,358000,359000,360000,361000,362000,363000,364000,365000,366000,367000,368000,369000,370000,371000,372000,373000,374000,375000,376000,377000,378000,379000,380000,381000,382000,383000,384000,385000,386000,387000,388000,389000,390000,391000,392000,393000,394000,395000,396000,397000,398000,399000,400000],"yaxis":"y","type":"scatter"}],                        {"template":{"data":{"histogram2dcontour":[{"type":"histogram2dcontour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"choropleth":[{"type":"choropleth","colorbar":{"outlinewidth":0,"ticks":""}}],"histogram2d":[{"type":"histogram2d","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmap":[{"type":"heatmap","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmapgl":[{"type":"heatmapgl","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"contourcarpet":[{"type":"contourcarpet","colorbar":{"outlinewidth":0,"ticks":""}}],"contour":[{"type":"contour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"surface":[{"type":"surface","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"mesh3d":[{"type":"mesh3d","colorbar":{"outlinewidth":0,"ticks":""}}],"scatter":[{"fillpattern":{"fillmode":"overlay","size":10,"solidity":0.2},"type":"scatter"}],"parcoords":[{"type":"parcoords","line":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolargl":[{"type":"scatterpolargl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"scattergeo":[{"type":"scattergeo","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolar":[{"type":"scatterpolar","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"scattergl":[{"type":"scattergl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatter3d":[{"type":"scatter3d","line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattermapbox":[{"type":"scattermapbox","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterternary":[{"type":"scatterternary","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattercarpet":[{"type":"scattercarpet","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"pie":[{"automargin":true,"type":"pie"}]},"layout":{"autotypenumbers":"strict","colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"hovermode":"closest","hoverlabel":{"align":"left"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"bgcolor":"#E5ECF6","angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"ternary":{"bgcolor":"#E5ECF6","aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"sequential":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"sequentialminus":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]]},"xaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"yaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"geo":{"bgcolor":"white","landcolor":"#E5ECF6","subunitcolor":"white","showland":true,"showlakes":true,"lakecolor":"white"},"title":{"x":0.05},"mapbox":{"style":"light"}}},"xaxis":{"anchor":"y","domain":[0.0,1.0],"title":{"text":"index"}},"yaxis":{"anchor":"x","domain":[0.0,1.0],"title":{"text":"value"},"type":"log"},"legend":{"title":{"text":"variable"},"tracegroupgap":0},"title":{"text":"Checkpoint Values for stanford-gpt2-small-a (Log scale)"}},                        {"responsive": true}                    ).then(function(){
                                     
         var gd = document.getElementById('87c7d210-cc47-4c30-be57-2c8152b6d491');
         var x = new MutationObserver(function (mutations, observer) {{
                 var display = window.getComputedStyle(gd).display;
                 if (!display || display === 'none') {{
                     console.log([gd, 'removed!']);
                     Plotly.purge(gd);
                     observer.disconnect();
                 }}
         }});

         // Listen for the removal of the full notebook cells
         var notebookContainer = gd.closest('#notebook-container');
         if (notebookContainer) {{
             x.observe(notebookContainer, {childList: true});
         }}

         // Listen for the clearing of the current output cell
         var outputEl = gd.closest('.output');
         if (outputEl) {{
             x.observe(outputEl, {childList: true});
         }}

                                 })                };                            </script>        </div>
         </body>
         </html>

   .. container:: output display_data

      .. raw:: html

         <html>
         <head><meta charset="utf-8" /></head>
         <body>
             <div>            <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG"></script><script type="text/javascript">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: "STIX-Web"}});}</script>                <script type="text/javascript">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>
                 <script src="https://cdn.plot.ly/plotly-2.16.1.min.js"></script>                <div id="3614b7fd-aa3e-4b49-a7d7-215d10df33e5" class="plotly-graph-div" style="height:525px; width:100%;"></div>            <script type="text/javascript">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById("3614b7fd-aa3e-4b49-a7d7-215d10df33e5")) {                    Plotly.newPlot(                        "3614b7fd-aa3e-4b49-a7d7-215d10df33e5",                        [{"hovertemplate":"variable=0<br>index=%{x}<br>value=%{y}<extra></extra>","legendgroup":"0","line":{"color":"#636efa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"markers+lines","name":"0","orientation":"v","showlegend":true,"x":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49],"xaxis":"x","y":[832,1664,2496,3328,4160,4992,5824,6656,7488,8320,9152,9984,10816,11648,12480,13312,14144,14976,15808,16640,17472,18304,19136,19968,20800,21632,22464,23296,24128,24960,25792,26624,27456,28288,29120,29952,30784,31616,32448,33280,34112,34944,35776,36608,37440,38272,39104,39936,40768,41600],"yaxis":"y","type":"scatter"}],                        {"template":{"data":{"histogram2dcontour":[{"type":"histogram2dcontour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"choropleth":[{"type":"choropleth","colorbar":{"outlinewidth":0,"ticks":""}}],"histogram2d":[{"type":"histogram2d","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmap":[{"type":"heatmap","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmapgl":[{"type":"heatmapgl","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"contourcarpet":[{"type":"contourcarpet","colorbar":{"outlinewidth":0,"ticks":""}}],"contour":[{"type":"contour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"surface":[{"type":"surface","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"mesh3d":[{"type":"mesh3d","colorbar":{"outlinewidth":0,"ticks":""}}],"scatter":[{"fillpattern":{"fillmode":"overlay","size":10,"solidity":0.2},"type":"scatter"}],"parcoords":[{"type":"parcoords","line":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolargl":[{"type":"scatterpolargl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"scattergeo":[{"type":"scattergeo","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolar":[{"type":"scatterpolar","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"scattergl":[{"type":"scattergl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatter3d":[{"type":"scatter3d","line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattermapbox":[{"type":"scattermapbox","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterternary":[{"type":"scatterternary","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattercarpet":[{"type":"scattercarpet","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"pie":[{"automargin":true,"type":"pie"}]},"layout":{"autotypenumbers":"strict","colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"hovermode":"closest","hoverlabel":{"align":"left"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"bgcolor":"#E5ECF6","angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"ternary":{"bgcolor":"#E5ECF6","aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"sequential":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"sequentialminus":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]]},"xaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"yaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"geo":{"bgcolor":"white","landcolor":"#E5ECF6","subunitcolor":"white","showland":true,"showlakes":true,"lakecolor":"white"},"title":{"x":0.05},"mapbox":{"style":"light"}}},"xaxis":{"anchor":"y","domain":[0.0,1.0],"title":{"text":"index"}},"yaxis":{"anchor":"x","domain":[0.0,1.0],"title":{"text":"value"}},"legend":{"title":{"text":"variable"},"tracegroupgap":0},"title":{"text":"Checkpoint Values for solu-1l-pile (Linear scale)"}},                        {"responsive": true}                    ).then(function(){
                                     
         var gd = document.getElementById('3614b7fd-aa3e-4b49-a7d7-215d10df33e5');
         var x = new MutationObserver(function (mutations, observer) {{
                 var display = window.getComputedStyle(gd).display;
                 if (!display || display === 'none') {{
                     console.log([gd, 'removed!']);
                     Plotly.purge(gd);
                     observer.disconnect();
                 }}
         }});

         // Listen for the removal of the full notebook cells
         var notebookContainer = gd.closest('#notebook-container');
         if (notebookContainer) {{
             x.observe(notebookContainer, {childList: true});
         }}

         // Listen for the clearing of the current output cell
         var outputEl = gd.closest('.output');
         if (outputEl) {{
             x.observe(outputEl, {childList: true});
         }}

                                 })                };                            </script>        </div>
         </body>
         </html>

   .. container:: output display_data

      .. raw:: html

         <html>
         <head><meta charset="utf-8" /></head>
         <body>
             <div>            <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG"></script><script type="text/javascript">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: "STIX-Web"}});}</script>                <script type="text/javascript">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>
                 <script src="https://cdn.plot.ly/plotly-2.16.1.min.js"></script>                <div id="440d9dd4-5051-445a-be32-9ad92e71921a" class="plotly-graph-div" style="height:525px; width:100%;"></div>            <script type="text/javascript">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById("440d9dd4-5051-445a-be32-9ad92e71921a")) {                    Plotly.newPlot(                        "440d9dd4-5051-445a-be32-9ad92e71921a",                        [{"hovertemplate":"variable=0<br>index=%{x}<br>value=%{y}<extra></extra>","legendgroup":"0","line":{"color":"#636efa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"markers+lines","name":"0","orientation":"v","showlegend":true,"x":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99],"xaxis":"x","y":[326,652,978,1304,1630,1956,2282,2608,2934,3260,3586,3912,4238,4564,4890,5216,5542,5868,6194,6520,6846,7172,7498,7824,8150,8476,8802,9128,9454,9780,10106,10432,10758,11084,11410,11736,12062,12388,12714,13040,13366,13692,14018,14344,14670,14996,15322,15648,15974,16300,16626,16952,17278,17604,17930,18256,18582,18908,19234,19560,19886,20212,20538,20864,21190,21516,21842,22168,22494,22820,23146,23472,23798,24124,24450,24776,25102,25428,25754,26080,26406,26732,27058,27384,27710,28036,28362,28688,29014,29340,29666,29992,30318,30644,30970,31296,31622,31948,32274,32600],"yaxis":"y","type":"scatter"}],                        {"template":{"data":{"histogram2dcontour":[{"type":"histogram2dcontour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"choropleth":[{"type":"choropleth","colorbar":{"outlinewidth":0,"ticks":""}}],"histogram2d":[{"type":"histogram2d","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmap":[{"type":"heatmap","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmapgl":[{"type":"heatmapgl","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"contourcarpet":[{"type":"contourcarpet","colorbar":{"outlinewidth":0,"ticks":""}}],"contour":[{"type":"contour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"surface":[{"type":"surface","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"mesh3d":[{"type":"mesh3d","colorbar":{"outlinewidth":0,"ticks":""}}],"scatter":[{"fillpattern":{"fillmode":"overlay","size":10,"solidity":0.2},"type":"scatter"}],"parcoords":[{"type":"parcoords","line":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolargl":[{"type":"scatterpolargl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"scattergeo":[{"type":"scattergeo","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolar":[{"type":"scatterpolar","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"scattergl":[{"type":"scattergl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatter3d":[{"type":"scatter3d","line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattermapbox":[{"type":"scattermapbox","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterternary":[{"type":"scatterternary","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattercarpet":[{"type":"scattercarpet","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"pie":[{"automargin":true,"type":"pie"}]},"layout":{"autotypenumbers":"strict","colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"hovermode":"closest","hoverlabel":{"align":"left"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"bgcolor":"#E5ECF6","angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"ternary":{"bgcolor":"#E5ECF6","aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"sequential":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"sequentialminus":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]]},"xaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"yaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"geo":{"bgcolor":"white","landcolor":"#E5ECF6","subunitcolor":"white","showland":true,"showlakes":true,"lakecolor":"white"},"title":{"x":0.05},"mapbox":{"style":"light"}}},"xaxis":{"anchor":"y","domain":[0.0,1.0],"title":{"text":"index"}},"yaxis":{"anchor":"x","domain":[0.0,1.0],"title":{"text":"value"}},"legend":{"title":{"text":"variable"},"tracegroupgap":0},"title":{"text":"Checkpoint Values for solu-6l-pile (Linear scale)"}},                        {"responsive": true}                    ).then(function(){
                                     
         var gd = document.getElementById('440d9dd4-5051-445a-be32-9ad92e71921a');
         var x = new MutationObserver(function (mutations, observer) {{
                 var display = window.getComputedStyle(gd).display;
                 if (!display || display === 'none') {{
                     console.log([gd, 'removed!']);
                     Plotly.purge(gd);
                     observer.disconnect();
                 }}
         }});

         // Listen for the removal of the full notebook cells
         var notebookContainer = gd.closest('#notebook-container');
         if (notebookContainer) {{
             x.observe(notebookContainer, {childList: true});
         }}

         // Listen for the clearing of the current output cell
         var outputEl = gd.closest('.output');
         if (outputEl) {{
             x.observe(outputEl, {childList: true});
         }}

                                 })                };                            </script>        </div>
         </body>
         </html>

.. container:: cell markdown

   .. rubric:: Example: Induction Head Phase Transition
      :name: example-induction-head-phase-transition

.. container:: cell markdown

   One of the more interesting results analysing circuit formation
   during training is the `induction head phase
   transition <https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html>`__.
   They find a pretty dramatic shift in models during training - there's
   a brief period where models go from not having induction heads to
   having them, which leads to the models suddenly becoming much better
   at in-context learning (using far back tokens to predict the next
   token, eg over 500 words back). This is enough of a big deal that it
   leads to a visible *bump* in the loss curve, where the model's rate
   of improvement briefly increases.

.. container:: cell markdown

   As a brief demonstration of the existence of the phase transition,
   let's load some checkpoints of a two layer model, and see whether
   they have induction heads. An easy test, as we used above, is to give
   the model a repeated sequence of random tokens, and to check how good
   its loss is on the second half. ``evals.induction_loss`` is a rough
   util that runs this test on a model. (Note - this is deliberately a
   rough, non-rigorous test for the purposes of demonstration, eg
   ``evals.induction_loss`` by default just runs it on 4 sequences of
   384 tokens repeated twice. These results totally don't do the paper
   justice - go check it out if you want to see the full results!)

.. container:: cell markdown

   In the interests of time and memory, let's look at a handful of
   checkpoints (chosen to be around the phase change), indices
   ``[10, 25, 35, 60, -1]``. These are roughly 22M, 200M, 500M, 1.6B and
   21.8B tokens through training, respectively. (I generally recommend
   looking things up based on indices, rather than checkpoint value!).

.. container:: cell code

   .. code:: python

      from transformer_lens import evals
      # We use the two layer model with SoLU activations, chosen fairly arbitrarily as being both small (so fast to download and keep in memory) and pretty good at the induction task.
      model_name = "solu-2l"
      # We can load a model from a checkpoint by specifying the checkpoint_index, -1 means the final checkpoint
      checkpoint_indices = [10, 25, 35, 60, -1]
      checkpointed_models = []
      tokens_trained_on = []
      induction_losses = []

.. container:: cell markdown

   We load the models, cache them in a list, and

.. container:: cell code

   .. code:: python

      for index in checkpoint_indices:
          # Load the model from the relevant checkpoint by index
          model_for_this_checkpoint = HookedTransformer.from_pretrained(model_name, checkpoint_index=index)
          checkpointed_models.append(model_for_this_checkpoint)

          tokens_seen_for_this_checkpoint = model_for_this_checkpoint.cfg.checkpoint_value
          tokens_trained_on.append(tokens_seen_for_this_checkpoint)

          induction_loss_for_this_checkpoint = evals.induction_loss(model_for_this_checkpoint).item()
          induction_losses.append(induction_loss_for_this_checkpoint)

   .. container:: output stream stdout

      ::

         Loaded pretrained model solu-2l into HookedTransformer
         Loaded pretrained model solu-2l into HookedTransformer
         Loaded pretrained model solu-2l into HookedTransformer
         Loaded pretrained model solu-2l into HookedTransformer
         Loaded pretrained model solu-2l into HookedTransformer

.. container:: cell markdown

   We can plot this, and see there's a sharp shift from ~200-500M tokens
   trained on (note the log scale on the x axis). Interestingly, this is
   notably earlier than the phase transition in the paper, I'm not sure
   what's up with that.

   (To contextualise the numbers, the tokens in the random sequence are
   uniformly chosen from the first 20,000 tokens (out of ~48,000 total),
   so random performance is at least :math:`\ln(20000)\approx 10`. A
   naive strategy like "randomly choose a token that's already appeared
   in the first half of the sequence (384 elements)" would get
   :math:`\ln(384)\approx 5.95`, so the model is doing pretty well
   here.)

.. container:: cell code

   .. code:: python

      line(induction_losses, x=tokens_trained_on, xaxis="Tokens Trained On", yaxis="Induction Loss", title="Induction Loss over training: solu-2l", markers=True, log_x=True)

   .. container:: output display_data

      .. raw:: html

         <html>
         <head><meta charset="utf-8" /></head>
         <body>
             <div>            <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG"></script><script type="text/javascript">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: "STIX-Web"}});}</script>                <script type="text/javascript">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>
                 <script src="https://cdn.plot.ly/plotly-2.16.1.min.js"></script>                <div id="6e70a192-5d22-459b-9b0e-f6d00be0bd1e" class="plotly-graph-div" style="height:525px; width:100%;"></div>            <script type="text/javascript">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById("6e70a192-5d22-459b-9b0e-f6d00be0bd1e")) {                    Plotly.newPlot(                        "6e70a192-5d22-459b-9b0e-f6d00be0bd1e",                        [{"hovertemplate":"Tokens Trained On=%{x}<br>index=%{y}<extra></extra>","legendgroup":"","line":{"color":"#636efa","dash":"solid"},"marker":{"symbol":"circle"},"mode":"markers+lines","name":"","orientation":"h","showlegend":false,"x":[22282240,187432960,528482304,1628438528,21780496384],"xaxis":"x","y":[0,1,2,3,4],"yaxis":"y","type":"scatter"}],                        {"template":{"data":{"histogram2dcontour":[{"type":"histogram2dcontour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"choropleth":[{"type":"choropleth","colorbar":{"outlinewidth":0,"ticks":""}}],"histogram2d":[{"type":"histogram2d","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmap":[{"type":"heatmap","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"heatmapgl":[{"type":"heatmapgl","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"contourcarpet":[{"type":"contourcarpet","colorbar":{"outlinewidth":0,"ticks":""}}],"contour":[{"type":"contour","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"surface":[{"type":"surface","colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]]}],"mesh3d":[{"type":"mesh3d","colorbar":{"outlinewidth":0,"ticks":""}}],"scatter":[{"fillpattern":{"fillmode":"overlay","size":10,"solidity":0.2},"type":"scatter"}],"parcoords":[{"type":"parcoords","line":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolargl":[{"type":"scatterpolargl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"bar":[{"error_x":{"color":"#2a3f5f"},"error_y":{"color":"#2a3f5f"},"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"scattergeo":[{"type":"scattergeo","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterpolar":[{"type":"scatterpolar","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"scattergl":[{"type":"scattergl","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatter3d":[{"type":"scatter3d","line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattermapbox":[{"type":"scattermapbox","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scatterternary":[{"type":"scatterternary","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"scattercarpet":[{"type":"scattercarpet","marker":{"colorbar":{"outlinewidth":0,"ticks":""}}}],"carpet":[{"aaxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"baxis":{"endlinecolor":"#2a3f5f","gridcolor":"white","linecolor":"white","minorgridcolor":"white","startlinecolor":"#2a3f5f"},"type":"carpet"}],"table":[{"cells":{"fill":{"color":"#EBF0F8"},"line":{"color":"white"}},"header":{"fill":{"color":"#C8D4E3"},"line":{"color":"white"}},"type":"table"}],"barpolar":[{"marker":{"line":{"color":"#E5ECF6","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"pie":[{"automargin":true,"type":"pie"}]},"layout":{"autotypenumbers":"strict","colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#2a3f5f"},"hovermode":"closest","hoverlabel":{"align":"left"},"paper_bgcolor":"white","plot_bgcolor":"#E5ECF6","polar":{"bgcolor":"#E5ECF6","angularaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"radialaxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"ternary":{"bgcolor":"#E5ECF6","aaxis":{"gridcolor":"white","linecolor":"white","ticks":""},"baxis":{"gridcolor":"white","linecolor":"white","ticks":""},"caxis":{"gridcolor":"white","linecolor":"white","ticks":""}},"coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"sequential":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"sequentialminus":[[0.0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1.0,"#f0f921"]],"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]]},"xaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"yaxis":{"gridcolor":"white","linecolor":"white","ticks":"","title":{"standoff":15},"zerolinecolor":"white","automargin":true,"zerolinewidth":2},"scene":{"xaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"yaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2},"zaxis":{"backgroundcolor":"#E5ECF6","gridcolor":"white","linecolor":"white","showbackground":true,"ticks":"","zerolinecolor":"white","gridwidth":2}},"shapedefaults":{"line":{"color":"#2a3f5f"}},"annotationdefaults":{"arrowcolor":"#2a3f5f","arrowhead":0,"arrowwidth":1},"geo":{"bgcolor":"white","landcolor":"#E5ECF6","subunitcolor":"white","showland":true,"showlakes":true,"lakecolor":"white"},"title":{"x":0.05},"mapbox":{"style":"light"}}},"xaxis":{"anchor":"y","domain":[0.0,1.0],"title":{"text":"Tokens Trained On"},"type":"log"},"yaxis":{"anchor":"x","domain":[0.0,1.0],"title":{"text":"index"}},"legend":{"tracegroupgap":0},"title":{"text":"Induction Loss over training: solu-2l"}},                        {"responsive": true}                    ).then(function(){
                                     
         var gd = document.getElementById('6e70a192-5d22-459b-9b0e-f6d00be0bd1e');
         var x = new MutationObserver(function (mutations, observer) {{
                 var display = window.getComputedStyle(gd).display;
                 if (!display || display === 'none') {{
                     console.log([gd, 'removed!']);
                     Plotly.purge(gd);
                     observer.disconnect();
                 }}
         }});

         // Listen for the removal of the full notebook cells
         var notebookContainer = gd.closest('#notebook-container');
         if (notebookContainer) {{
             x.observe(notebookContainer, {childList: true});
         }}

         // Listen for the clearing of the current output cell
         var outputEl = gd.closest('.output');
         if (outputEl) {{
             x.observe(outputEl, {childList: true});
         }}

                                 })                };                            </script>        </div>
         </body>
         </html>
